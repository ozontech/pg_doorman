{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pgdoorman-postgresql-pooler","title":"PgDoorman: PostgreSQL Pooler","text":"<p>PgDoorman is a stable and high-performance alternative to PgBouncer, Odyssey, or PgCat. It was created with the Unix philosophy in mind. Development focused on performance, efficiency, and reliability. Additionally, PgDoorman provides improved driver support for languages like Go (pgx), .NET (npgsql), and asynchronous drivers for Python and Node.js.</p> <p> Get PgDoorman 3.2.0</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Run PgDoorman instantly using Docker:</p> <pre><code>docker run -p 6432:6432 \\\n  -v $(pwd)/pg_doorman.toml:/etc/pg_doorman/pg_doorman.toml \\\n  ghcr.io/ozontech/pg_doorman\n</code></pre> <p>For more details, see the Installation Guide.</p>"},{"location":"#why-not-multi-pgbouncer","title":"Why not multi-PgBouncer?","text":"<p>Why do we think that using multiple instances of PgBouncer is not a suitable solution? This approach has problems with reusing prepared statements and strange and inefficient control over query cancellation. Additionally, the main issue we have encountered is that the operating system distributes new clients round-robin, but each client can disconnect at any time, leading to an imbalance after prolonged use.</p>"},{"location":"#why-not-odyssey","title":"Why not Odyssey?","text":"<p>We had difficulties using NPGSQL and SCRAM, as well as with <code>prepared_statements</code> support. However, the main serious problem related to data consistency and, for a long time, we were unable to solve it.</p>"},{"location":"#differences-from-pgcat","title":"Differences from PgCat","text":"<p>While PgDoorman was initially based on the PgCat project, it has since evolved into a standalone solution with its own set of features. Some of the key differences include:</p> <ul> <li>Performance improvements compared to PgCat/PgBouncer/Odyssey.</li> <li>Support for extended protocol with popular programming language drivers.</li> <li>Enhanced monitoring metrics to improve visibility into database activity.</li> <li>Careful resource management to avoid memory issues (<code>max_memory_usage</code>, <code>message_size_to_be_stream</code>).</li> <li>SCRAM client/server authentication support.</li> <li>Gracefully binary upgrade.</li> <li>Supporting JWT for service-to-database authentication.</li> <li>Many micro-optimizations (for example, the time spent with the client is longer than the server's busy time).</li> </ul>"},{"location":"#additional-binary-patroni_proxy","title":"Additional Binary: patroni_proxy","text":"<p>This repository also includes patroni_proxy \u2014 a specialized high-performance TCP proxy for Patroni-managed PostgreSQL clusters. Unlike HAProxy + confd, it preserves existing connections during cluster topology changes and provides native role-based routing with replication lag awareness.</p>"},{"location":"benchmarks/","title":"Performance Benchmarks","text":""},{"location":"benchmarks/#automated-benchmark-results","title":"Automated Benchmark Results","text":"<p>Last updated: 2026-02-04 08:38 UTC</p> <p>These benchmarks are automatically generated by the CI pipeline using <code>pgbench</code>.</p>"},{"location":"benchmarks/#test-environment","title":"Test Environment","text":"<ul> <li>Pool size: 40 connections</li> <li>Test duration: 30 seconds per test</li> <li>Instance: AWS Fargate (16 vCPU, 32 GB RAM)</li> <li>Workers: pg_doorman: 12, odyssey: 12</li> <li>pgbench jobs: 4 (global override)</li> <li>Started: 2026-02-04 07:15:48 UTC</li> <li>Finished: 2026-02-04 08:38:55 UTC</li> <li>Total duration: 1h 23m 7s</li> </ul>"},{"location":"benchmarks/#legend","title":"Legend","text":"<ul> <li>+N%: pg_doorman is N% faster than competitor (e.g., +10% means pg_doorman is 10% faster)</li> <li>-N%: pg_doorman is N% slower than competitor (e.g., -10% means pg_doorman is 10% slower)</li> <li>\u22480%: Equal performance (difference less than 3%)</li> <li>\u221e: Competitor failed (0 TPS), pg_doorman wins</li> <li>N/A: Test not supported by this pooler</li> <li>-: Test not executed</li> </ul>"},{"location":"benchmarks/#simple-protocol","title":"Simple Protocol","text":"Test vs pgbouncer vs odyssey 1 client -5% -10% 40 clients +37% -41% 120 clients x3.0 -7% 500 clients x2.8 \u22480% 10,000 clients x2.8 +20% 1 client + Reconnect \u22480% x3.5 40 clients + Reconnect +21% x2.4 120 clients + Reconnect +21% x2.1 500 clients + Reconnect +21% x2.2 10,000 clients + Reconnect +67% x3.3 1 client + SSL -7% -11% 40 clients + SSL +69% -30% 120 clients + SSL x3.3 -4% 500 clients + SSL x3.3 +3% 10,000 clients + SSL x3.4 +19%"},{"location":"benchmarks/#extended-protocol","title":"Extended Protocol","text":"Test vs pgbouncer vs odyssey 1 client \u22480% +33% 40 clients +49% -11% 120 clients x3.0 +43% 500 clients x2.9 +55% 10,000 clients x2.9 +80% 1 client + Reconnect \u22480% x10.5 40 clients + Reconnect +21% x2.7 120 clients + Reconnect +22% x2.3 500 clients + Reconnect +21% x2.1 10,000 clients + Reconnect +68% x2.2 1 client + SSL \u22480% +33% 40 clients + SSL +72% \u22480% 120 clients + SSL x3.4 +48% 500 clients + SSL x3.5 +61% 10,000 clients + SSL x3.5 +82% 1 client + SSL + Reconnect +6% +11% 40 clients + SSL + Reconnect +94% \u22480% 120 clients + SSL + Reconnect +92% \u22480% 500 clients + SSL + Reconnect +96% \u22480% 10,000 clients + SSL + Reconnect +90% \u22480%"},{"location":"benchmarks/#prepared-protocol","title":"Prepared Protocol","text":"Test vs pgbouncer vs odyssey 1 client -8% -13% 40 clients +70% -42% 120 clients x3.7 -10% 500 clients x3.4 \u22480% 10,000 clients x3.4 +21% 1 client + Reconnect \u22480% \u221e 40 clients + Reconnect \u22480% \u221e 120 clients + Reconnect +4% \u221e 500 clients + Reconnect +6% \u221e 10,000 clients + Reconnect +28% \u221e 1 client + SSL \u22480% -8% 40 clients + SSL +97% -34% 120 clients + SSL x3.9 -7% 500 clients + SSL x4.0 +5% 10,000 clients + SSL x4.0 +22%"},{"location":"benchmarks/#notes","title":"Notes","text":"<ul> <li>Odyssey has poor support for extended query protocol in transaction pooling mode, resulting in significantly lower performance compared to pg_doorman and pgbouncer</li> <li>Important: The values shown are relative performance ratios, not absolute TPS numbers. While absolute TPS values may vary depending on hardware and system load, the relative ratios between poolers should remain consistent when tests are run sequentially in a short timeframe (30 seconds each). This allows for fair comparison across different connection poolers under identical conditions</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#3.2.0","title":"3.2.0 Feb 3, 2026","text":"<p>New Features:</p> <ul> <li> <p>Configuration test mode (<code>-t</code> / <code>--test-config</code>): Added nginx-style configuration validation flag. Running <code>pg_doorman -t</code> or <code>pg_doorman --test-config</code> will parse and validate the configuration file, report success or errors, and exit without starting the server. Useful for CI/CD pipelines and pre-deployment configuration checks.</p> </li> <li> <p>Configuration validation before binary upgrade: When receiving SIGINT for graceful shutdown/binary upgrade, the server now validates the new binary's configuration using <code>-t</code> flag before proceeding. If the configuration test fails, the shutdown is cancelled and critical error messages are logged to alert the operator. This prevents accidental downtime from deploying a binary with invalid configuration.</p> </li> <li> <p>New <code>retain_connections_max</code> configuration parameter: Controls the maximum number of idle connections to close per retain cycle. When set to <code>0</code> (default), all idle connections that exceed <code>idle_timeout</code> or <code>server_lifetime</code> are closed immediately, ensuring aggressive cleanup of unused connections. Previously, only 1 connection was closed per cycle, which could lead to slow connection cleanup when many connections became idle simultaneously. Connection closures are now logged for better observability.</p> </li> <li> <p>Changed default for <code>retain_connections_time</code>: The default interval for the connection retain task has been reduced from 60 seconds to 30 seconds, providing faster cleanup of expired idle connections.</p> </li> <li> <p>New <code>tcp_user_timeout</code> configuration parameter: Sets the <code>TCP_USER_TIMEOUT</code> socket option for client connections (in seconds). This helps detect dead client connections faster than keepalive probes when the connection is actively sending data but the remote end has become unreachable. Prevents 15-16 minute delays caused by TCP retransmission timeout. Only supported on Linux. Set to <code>0</code> to disable (default).</p> </li> <li> <p>New <code>tcp_user_timeout</code> configuration parameter: Sets the <code>TCP_USER_TIMEOUT</code> socket option for client connections (in seconds). This helps detect dead client connections faster than keepalive probes when the connection is actively sending data but the remote end has become unreachable. Prevents 15-16 minute delays caused by TCP retransmission timeout. Only supported on Linux. Set to <code>0</code> to disable (default).</p> </li> </ul> <p>Simplification:</p> <ul> <li> <p>Removed <code>wait_rollback</code> mechanism: The pooler no longer attempts to automatically wait for ROLLBACK from clients when a transaction enters an aborted state. This complex mechanism was causing protocol desynchronization issues with async clients and extended query protocol. Server connections in aborted transactions are now simply returned to the pool and cleaned up normally via ROLLBACK during checkin.</p> </li> <li> <p>Removed savepoint tracking: Removed the <code>use_savepoint</code> flag and related logic that was tracking SAVEPOINT usage. The pooler now treats savepoints as regular PostgreSQL commands without special handling.</p> </li> </ul> <p>Bug Fixes:</p> <ul> <li>Fixed protocol desynchronization in async mode with simple prepared statements: When <code>prepared_statements</code> was disabled but clients used extended query protocol (Parse, Bind, Describe, Execute, Flush), the pooler wasn't tracking batch operations, causing <code>expected_responses</code> to be calculated as 0. This led to the pooler exiting the response loop immediately without waiting for server responses (ParseComplete, BindComplete, etc.). Now batch operations are tracked regardless of the <code>prepared_statements</code> setting.</li> </ul> <p>Performance:</p> <ul> <li>Removed timeout-based waiting in async protocol: The pooler now tracks expected responses based on batch operations (Parse, Bind, Execute, etc.) and exits immediately when all responses are received. This eliminates unnecessary latency in pipeline/async workloads.</li> </ul>"},{"location":"changelog/#3.1.8","title":"3.1.8 Jan 31, 2026","text":"<p>Bug Fixes:</p> <ul> <li> <p>Fixed ParseComplete desynchronization in pipeline on errors: Fixed a protocol desynchronization issue (especially noticeable in .NET Npgsql driver) where synthetic <code>ParseComplete</code> messages were not being inserted if an error occurred during a pipelined batch. When the pooler caches a prepared statement and skips sending <code>Parse</code> to the server, it must still provide a <code>ParseComplete</code> to the client. If an error occurs before subsequent commands are processed, the server skips them, and the pooler now ensures all missing synthetic <code>ParseComplete</code> messages are inserted into the response stream upon receiving an <code>ErrorResponse</code> or <code>ReadyForQuery</code>.</p> </li> <li> <p>Fixed incorrect <code>use_savepoint</code> state persistence: Fixed a bug where the <code>use_savepoint</code> flag (which disables automatic rollback on connection return if a savepoint was used) was not reset after a transaction ended.</p> </li> </ul>"},{"location":"changelog/#3.1.7","title":"3.1.7 Jan 28, 2026","text":"<p>Memory Optimization:</p> <ul> <li> <p>DEALLOCATE now clears client prepared statements cache: When a client sends <code>DEALLOCATE &lt;name&gt;</code> or <code>DEALLOCATE ALL</code> via simple query protocol, the pooler now properly clears the corresponding entries from the client's internal prepared statements cache. Previously, synthetic OK responses were sent but the client cache was not cleared, causing memory to grow indefinitely for long-running connections using many unique prepared statements. This fix allows memory to be reclaimed when clients properly deallocate their statements.</p> </li> <li> <p>New <code>client_prepared_statements_cache_size</code> configuration parameter: Added protection against malicious or misbehaving clients that don't call <code>DEALLOCATE</code> and could exhaust server memory by creating unlimited prepared statements. When the per-client cache limit is reached, the oldest entry is evicted automatically. Set to <code>0</code> for unlimited (default, relies on client calling <code>DEALLOCATE</code>). Example: <code>client_prepared_statements_cache_size: 1024</code> limits each client to 1024 cached prepared statements.</p> </li> </ul>"},{"location":"changelog/#3.1.6","title":"3.1.6 Jan 27, 2026","text":"<p>Bug Fixes:</p> <ul> <li> <p>Fixed incorrect timing statistics (xact_time, wait_time, percentiles): The statistics module was using <code>recent()</code> (cached clock) without proper clock cache updates, causing transaction time, wait time, and their percentiles to show extremely large incorrect values (e.g., 100+ seconds instead of actual milliseconds). The root cause was that the <code>quanta::Upkeep</code> handle was not being stored, causing the upkeep thread to stop immediately after starting. Now the handle is properly retained for the lifetime of the server, ensuring <code>Clock::recent()</code> returns accurate cached time values.</p> </li> <li> <p>Fixed query time accumulation bug in transaction loop: Query times were incorrectly accumulated when multiple queries were executed within a single transaction. The <code>query_start_at</code> timestamp was only set once at the beginning of the transaction, causing each subsequent query's elapsed time to include all previous queries' durations (e.g., 10 queries of 100ms each would report the last query as ~1 second instead of 100ms). Now <code>query_start_at</code> is updated for each new message in the transaction loop, ensuring accurate per-query timing.</p> </li> </ul> <p>New Features:</p> <ul> <li> <p>New <code>clock_resolution_statistics</code> configuration parameter: Added <code>general.clock_resolution_statistics</code> parameter (default: <code>0.1ms</code> = 100 microseconds) that controls how often the internal clock cache is updated. Lower values provide more accurate timing measurements for query/transaction percentiles, while higher values reduce CPU overhead. This parameter affects the accuracy of all timing statistics reported in the admin console and Prometheus metrics.</p> </li> <li> <p>Sub-millisecond precision for Duration values: Duration configuration parameters now support sub-millisecond precision:</p> </li> <li>New <code>us</code> suffix for microseconds (e.g., <code>\"100us\"</code> = 100 microseconds)</li> <li>Decimal milliseconds support (e.g., <code>\"0.1ms\"</code> = 100 microseconds)</li> <li>Internal representation changed from milliseconds to microseconds for higher precision</li> <li>Full backward compatibility maintained: plain numbers are still interpreted as milliseconds</li> </ul>"},{"location":"changelog/#3.1.5","title":"3.1.5 Jan 25, 2026","text":"<p>Bug Fixes:</p> <ul> <li>Fixed PROTOCOL VIOLATION with batch PrepareAsync</li> <li>Rewritten ParseComplete insertion algorithm</li> </ul> <p>Performance:</p> <ul> <li>Deferred connection acquisition for standalone BEGIN: When a client sends a standalone <code>BEGIN;</code> or <code>begin;</code> query (simple query protocol), the pooler now defers acquiring a server connection until the next message arrives. Since <code>BEGIN</code> itself doesn't perform any actual database operations, this optimization reduces connection pool contention when clients are slow to send their next query after starting a transaction.</li> <li>Micro-optimized detection: first checks message size (12 bytes), then content using case-insensitive comparison</li> <li>If client sends Terminate (<code>X</code>) after <code>BEGIN</code>, no server connection is acquired at all</li> <li>The deferred <code>BEGIN</code> is automatically sent to the server before the actual query</li> </ul>"},{"location":"changelog/#3.1.0","title":"3.1.0 Jan 18, 2026","text":"<p>New Features:</p> <ul> <li>YAML configuration support: Added support for YAML configuration files (<code>.yaml</code>, <code>.yml</code>) as the primary and recommended format. The format is automatically detected based on file extension. TOML format remains fully supported for backward compatibility.</li> <li>The <code>generate</code> command now outputs YAML or TOML based on the output file extension.</li> <li>Include files can mix YAML and TOML formats.</li> <li>New array syntax for users in YAML: <code>users: [{ username: \"user1\", ... }]</code></li> <li>TOML backward compatibility: Full backward compatibility with legacy TOML format <code>[pools.*.users.0]</code> is maintained. Both the legacy map format and the new array format <code>[[pools.*.users]]</code> are supported.</li> <li>Username uniqueness validation: Added validation to reject duplicate usernames within a pool, ensuring configuration correctness.</li> <li>Human-readable configuration values: Duration and byte size parameters now support human-readable formats while maintaining backward compatibility with numeric values:</li> <li>Duration: <code>\"3s\"</code>, <code>\"5m\"</code>, <code>\"1h\"</code>, <code>\"1d\"</code> (or milliseconds: <code>3000</code>)</li> <li>Byte size: <code>\"1MB\"</code>, <code>\"256M\"</code>, <code>\"1GB\"</code> (or bytes: <code>1048576</code>)</li> <li>Example: <code>connect_timeout: \"3s\"</code> instead of <code>connect_timeout: 3000</code></li> <li>Foreground mode binary upgrade: Added support for binary upgrade in foreground mode by passing the listener socket to the new process via <code>--inherit-fd</code> argument. This enables zero-downtime upgrades without requiring daemon mode.</li> <li>Optional tokio runtime parameters: The following tokio runtime parameters are now optional and default to <code>None</code> (using tokio's built-in defaults): <code>tokio_global_queue_interval</code>, <code>tokio_event_interval</code>, <code>worker_stack_size</code>, and the new <code>max_blocking_threads</code>. Modern tokio versions handle these parameters well by default, so explicit configuration is no longer required in most cases.</li> <li>Improved graceful shutdown behavior:</li> <li>During graceful shutdown, only clients with active transactions are now counted (instead of all connected clients), allowing faster shutdown when clients are idle.</li> <li>After a client completes their transaction during shutdown, they receive a proper PostgreSQL protocol error (<code>58006 - pooler is shut down now</code>) instead of a connection reset.</li> <li>Server connections are immediately released (marked as bad) after transaction completion during shutdown to conserve PostgreSQL connections.</li> <li>All idle connections are immediately drained from pools when graceful shutdown starts, releasing PostgreSQL connections faster.</li> </ul> <p>Performance:</p> <ul> <li>Statistics module optimization: Major refactoring of the <code>src/stats</code> module for improved performance:</li> <li>Replaced <code>VecDeque</code> with HDR histograms (<code>hdrhistogram</code> crate) for percentile calculations \u2014 O(1) percentile queries instead of O(n log n) sorting, ~95% memory reduction for latency tracking.</li> <li>Histograms are now reset after each stats period (15 seconds) to provide accurate rolling window percentiles.</li> </ul>"},{"location":"changelog/#3.0.5","title":"3.0.5 Jan 16, 2026","text":"<p>Bug Fixes:</p> <ul> <li>Fixed panic (<code>capacity overflow</code>) in startup message handling when receiving malformed messages with invalid length (less than 8 bytes or exceeding 10MB). Now gracefully rejects such connections with <code>ClientBadStartup</code> error.</li> </ul> <p>Testing:</p> <ul> <li>Integration fuzz testing framework: Added comprehensive BDD-based fuzz tests (<code>@fuzz</code> tag) that verify pg_doorman's resilience to malformed PostgreSQL protocol messages.</li> <li>All fuzz tests connect and authenticate first, then send malformed data to test post-authentication resilience.</li> </ul> <p>CI/CD:</p> <ul> <li>Added dedicated fuzz test job in GitHub Actions workflow (without retries, as fuzz tests should not be flaky).</li> </ul>"},{"location":"changelog/#3.0.4","title":"3.0.4 Jan 16, 2026","text":"<p>New Features:</p> <ul> <li>Enhanced DEBUG logging for PostgreSQL protocol messages: Added grouped debug logging that displays message types in a compact format (e.g., <code>[P(stmt1),B,D,E,S]</code> or <code>[3xD,C,Z]</code>). Messages are buffered and flushed every 100ms or 100 messages to reduce log noise.</li> <li>Protocol violation detection: Added real-time protocol state tracking that detects and warns about protocol violations (e.g., receiving ParseComplete when no Parse was pending). Helps diagnose client-server synchronization issues.</li> </ul> <p>Bug Fixes:</p> <ul> <li>Fixed potential protocol violation when client disconnects during batch operations with cached prepared statements: disabled fast_release optimization when there are pending prepared statement operations.</li> <li>Fixed ParseComplete insertion for Describe flow: now correctly inserts one ParseComplete before each ParameterDescription ('t') or NoData ('n') message instead of inserting all at once.</li> </ul>"},{"location":"changelog/#3.0.3","title":"3.0.3 Jan 15, 2026","text":"<p>Bug Fixes:</p> <ul> <li>Improved handling of Describe flow for cached prepared statements: added a separate counter (<code>pending_parse_complete_for_describe</code>) to correctly insert ParseComplete messages before ParameterDescription or NoData responses when Parse was skipped due to caching.</li> </ul> <p>Testing:</p> <ul> <li>Added comprehensive .NET client tests for Describe flow with cached prepared statements (<code>describe_flow_cached.cs</code>).</li> <li>Added aggressive mixed tests combining batch operations, prepared statements, and extended protocol (<code>aggressive_mixed.cs</code>).</li> </ul>"},{"location":"changelog/#3.0.2","title":"3.0.2 Jan 14, 2026","text":"<p>Bug Fixes:</p> <ul> <li>Fixed protocol mismatch for .NET clients (Npgsql) using named prepared statements with <code>Prepare()</code>: ParseComplete messages are now correctly inserted before ParameterDescription and NoData messages in the Describe flow, not just before BindComplete.</li> </ul>"},{"location":"changelog/#3.0.1","title":"3.0.1 Jan 14, 2026","text":"<p>Bug Fixes:</p> <ul> <li>Fixed protocol mismatch for .NET clients (Npgsql): prevented insertion of ParseComplete messages between DataRow messages when server has more data available.</li> </ul> <p>Testing:</p> <ul> <li>Extended Node.js client test coverage with additional scenarios for prepared statements, error handling, transactions, and edge cases.</li> </ul>"},{"location":"changelog/#3.0.0","title":"3.0.0 Jan 12, 2026","text":"<p>Major Release \u2014 Complete Architecture Refactoring</p> <p>This release represents a significant milestone with a complete codebase refactoring that dramatically improves async protocol support, making PgDoorman the most efficient connection pooler for asynchronous PostgreSQL workloads.</p> <p>New Features:</p> <ul> <li>patroni_proxy \u2014 A new high-performance TCP proxy for Patroni-managed PostgreSQL clusters:<ul> <li>Zero-downtime connection management \u2014 existing connections are preserved during cluster topology changes</li> <li>Hot upstream updates \u2014 automatic discovery of cluster members via Patroni REST API without connection drops</li> <li>Role-based routing \u2014 route connections to leader, sync replicas, or async replicas based on configuration</li> <li>Replication lag awareness with configurable <code>max_lag_in_bytes</code> per port</li> <li>Least connections load balancing strategy</li> </ul> </li> </ul> <p>Improvements:</p> <ul> <li>Complete codebase refactoring \u2014 modular architecture with better separation of concerns:<ul> <li>Client handling split into dedicated modules (core, entrypoint, protocol, startup, transaction)</li> <li>Configuration system reorganized into focused modules (general, pool, user, tls, prometheus, talos)</li> <li>Admin, auth, and prometheus subsystems extracted into separate modules</li> <li>Improved code maintainability and testability</li> </ul> </li> <li>Enhanced async protocol support \u2014 significantly improved handling of asynchronous PostgreSQL protocol, providing better performance than other connection poolers for async workloads</li> <li>Extended protocol improvements \u2014 better client buffering and message handling for extended query protocol</li> <li>xxhash3 for prepared statement hashing \u2014 faster hash computation for prepared statement cache</li> <li>Comprehensive BDD testing framework \u2014 multi-language integration tests (Go, Rust, Python, Node.js, .NET) with Docker-based reproducible environment</li> </ul>"},{"location":"changelog/#2.5.0","title":"2.5.0 Nov 18, 2025","text":"<p>Improvements: - Reworked the statistics collection system, yielding up to 20% performance gain on fast queries. - Improved detection of <code>SAVEPOINT</code> usage, allowing the auto-rollback feature to be applied in more situations.</p> <p>Bug Fixes / Behavior: - Less aggressive behavior on write errors when sending a response to the client: the server connection is no longer immediately marked as \"bad\" and evicted from the pool. We now read the remaining server response and clean up its state, returning the connection to the pool in a clean state. This improves performance during client reconnections.</p>"},{"location":"changelog/#2.4.3","title":"2.4.3 Nov 15, 2025","text":"<p>Bug Fixes: - Fixed handling of nested transactions via <code>SAVEPOINT</code>: auto-rollback now correctly rolls back to the savepoint instead of breaking the outer transaction. This prevents clients from getting stuck in an inconsistent transactional state.</p>"},{"location":"changelog/#2.4.2","title":"2.4.2 Nov 13, 2025","text":"<p>Improvements: - <code>pg_hba</code> rules now apply to the admin console as well; the <code>trust</code> method can be used for admin connections when a matching rule is present (use with caution; restrict by address/TLS).</p> <p>Bug Fixes: - Fixed <code>pg_hba</code> evaluation: <code>local</code> records were mistakenly considered; PgDoorman only handles TCP connections, so <code>local</code> entries are now correctly ignored.</p>"},{"location":"changelog/#2.4.1","title":"2.4.1 Nov 12, 2025","text":"<p>Improvements: - Performance optimizations in request handling and message processing paths to reduce latency and CPU usage. - <code>pg_hba</code> rules now apply to the admin console as well; the <code>trust</code> method can be used for admin connections when a matching rule is present (use with caution; restrict by address/TLS).</p> <p>Bug Fixes: - Corrected logic where <code>COMMIT</code> could be mishandled similarly to <code>ROLLBACK</code> in certain error states; now transactional state handling is aligned with PostgreSQL semantics.</p>"},{"location":"changelog/#2.4.0","title":"2.4.0 Nov 10, 2025","text":"<p>Features: - Added <code>pg_hba</code> support to control client access in PostgreSQL format. New <code>general.pg_hba</code> setting supports inline content or file path. - Clients that enter the <code>aborted in transaction</code> state are detached from their server backend; the proxy waits for the client to send <code>ROLLBACK</code>.</p> <p>Improvements: - Refined admin and metrics counters: separated <code>cancel</code> connections and corrected calculation of <code>error</code> connections in admin output and Prometheus metrics descriptions. - Added configuration validation to prevent simultaneous use of legacy <code>general.hba</code> CIDR list with the new <code>general.pg_hba</code> rules. - Improved validation and error messages for Talos token authentication.</p>"},{"location":"changelog/#2.2.2","title":"2.2.2 Aug 17, 2025","text":"<p>Features: - Added new generate feature functionality</p> <p>Bug Fixes: - Fixed deallocate issues with PGX5 compatibility</p>"},{"location":"changelog/#2.2.1","title":"2.2.1 Aug 6, 2025","text":"<p>Features: - Improve Prometheus exporter functionality</p>"},{"location":"changelog/#2.2.0","title":"2.2.0 Aug 5, 2025","text":"<p>Features: - Added Prometheus exporter functionality that provides metrics about connections, memory usage, pools, queries, and transactions</p>"},{"location":"changelog/#2.1.2","title":"2.1.2 Aug 4, 2025","text":"<p>Features: - Added docker image <code>ghcr.io/ozontech/pg_doorman</code></p>"},{"location":"changelog/#2.1.0","title":"2.1.0 Aug 1, 2025","text":"<p>Features: - The new command <code>generate</code> connects to your PostgreSQL server, automatically detects all databases and users, and creates a complete configuration file with appropriate settings. This is especially useful for quickly setting up PgDoorman in new environments or when you have many databases and users to configure.</p>"},{"location":"changelog/#2.0.1","title":"2.0.1 July 24, 2025","text":"<p>Bug Fixes: - Fixed <code>max_memory_usage</code> counter leak when clients disconnect improperly.</p>"},{"location":"changelog/#2.0.0","title":"2.0.0 July 22, 2025","text":"<p>Features: - Added <code>tls_mode</code> configuration option to enhance security with flexible TLS connection management and client certificate validation capabilities.</p>"},{"location":"changelog/#1.9.0","title":"1.9.0 July 20, 2025","text":"<p>Features: - Added PAM authentication support. - Added <code>talos</code> JWT authentication support.</p> <p>Improvements: - Implemented streaming for COPY protocol with large columns to prevent memory exhaustion. - Updated Rust and Tokio dependencies.</p>"},{"location":"changelog/#1.8.3","title":"1.8.3 Jun 11, 2025","text":"<p>Bug Fixes: - Fixed critical bug where Client's buffer wasn't cleared when no free connections were available in the Server pool (query_wait_timeout), leading to incorrect response errors. #38 - Fixed Npgsql-related issue. Npgsql#6115</p>"},{"location":"changelog/#1.8.2","title":"1.8.2 May 24, 2025","text":"<p>Features: - Added <code>application_name</code> parameter in pool. #30 - Added support for <code>DISCARD ALL</code> and <code>DEALLOCATE ALL</code> client queries.</p> <p>Improvements: - Implemented link-time optimization. #29</p> <p>Bug Fixes: - Fixed panics in admin console. - Fixed connection leakage on improperly handled errors in client's copy mode.</p>"},{"location":"changelog/#1.8.1","title":"1.8.1 April 12, 2025","text":"<p>Bug Fixes: - Fixed config value of prepared_statements. #21 - Fixed handling of declared cursors closure. #23 - Fixed proxy server parameters. #25</p>"},{"location":"changelog/#1.8.0","title":"1.8.0 Mar 20, 2025","text":"<p>Bug Fixes: - Fixed dependencies issue. #15</p> <p>Improvements: - Added release vendor-licenses.txt file. Related thread</p>"},{"location":"changelog/#1.7.9","title":"1.7.9 Mar 16, 2025","text":"<p>Improvements: - Added release vendor.tar.gz for offline build. Related thread</p> <p>Bug Fixes: - Fixed issues with pqCancel messages over TLS protocol. Drivers should send pqCancel messages exclusively via TLS if the primary connection was established using TLS. Npgsql follows this rule, while PGX currently does not. Both behaviors are now supported.</p>"},{"location":"changelog/#1.7.8","title":"1.7.8 Mar 8, 2025","text":"<p>Bug Fixes: - Fixed message ordering issue when using batch processing with the extended protocol. - Improved error message detail in logs for server-side login attempt failures.</p>"},{"location":"changelog/#1.7.7","title":"1.7.7 Mar 8, 2025","text":"<p>Features: - Enhanced <code>show clients</code> command with new fields: <code>state</code> (waiting/idle/active) and <code>wait</code> (read/write/idle). - Enhanced <code>show servers</code> command with new fields: <code>state</code> (login/idle/active), <code>wait</code> (read/write/idle), and <code>server_process_pid</code>. - Added 15-second proxy timeout for streaming large <code>message_size_to_be_stream</code> responses.</p> <p>Bug Fixes: - Fixed <code>max_memory_usage</code> counter leak when clients disconnect improperly.</p>"},{"location":"reference/general/","title":"Settings","text":""},{"location":"reference/general/#configuration-file-format","title":"Configuration File Format","text":"<p>pg_doorman supports two configuration file formats:</p> <ul> <li>YAML (<code>.yaml</code>, <code>.yml</code>) - The primary and recommended format for new configurations.</li> <li>TOML (<code>.toml</code>) - Supported for backward compatibility with existing configurations.</li> </ul> <p>The format is automatically detected based on the file extension. Both formats support the same configuration options and can be used interchangeably.</p>"},{"location":"reference/general/#example-yaml-configuration-recommended","title":"Example YAML Configuration (Recommended)","text":"<pre><code>general:\n  host: \"0.0.0.0\"\n  port: 6432\n  admin_username: \"admin\"\n  admin_password: \"admin\"\n\npools:\n  mydb:\n    server_host: \"localhost\"\n    server_port: 5432\n    pool_mode: \"transaction\"\n    users:\n      - username: \"myuser\"\n        password: \"mypassword\"\n        pool_size: 40\n</code></pre>"},{"location":"reference/general/#example-toml-configuration-legacy","title":"Example TOML Configuration (Legacy)","text":"<pre><code>[general]\nhost = \"0.0.0.0\"\nport = 6432\nadmin_username = \"admin\"\nadmin_password = \"admin\"\n\n[pools.mydb]\nserver_host = \"localhost\"\nserver_port = 5432\npool_mode = \"transaction\"\n\n[[pools.mydb.users]]\nusername = \"myuser\"\npassword = \"mypassword\"\npool_size = 40\n</code></pre>"},{"location":"reference/general/#generate-command","title":"Generate Command","text":"<p>The <code>generate</code> command can output configuration in either format. The format is determined by the output file extension:</p> <pre><code># Generate YAML configuration (recommended)\npg_doorman generate --output config.yaml\n\n# Generate TOML configuration (for backward compatibility)\npg_doorman generate --output config.toml\n</code></pre>"},{"location":"reference/general/#include-files","title":"Include Files","text":"<p>Include files can be in either format, and you can mix formats. For example, a YAML main config can include TOML files and vice versa:</p> <pre><code>include:\n  files:\n    - \"pools.yaml\"\n    - \"users.toml\"\n</code></pre>"},{"location":"reference/general/#human-readable-values","title":"Human-Readable Values","text":"<p>pg_doorman supports human-readable formats for duration and byte size values, while maintaining backward compatibility with numeric values.</p>"},{"location":"reference/general/#duration-format","title":"Duration Format","text":"<p>Duration values can be specified as:</p> <ul> <li>Plain numbers: interpreted as milliseconds (e.g., <code>5000</code> = 5 seconds)</li> <li>String with suffix:<ul> <li><code>ms</code> - milliseconds (e.g., <code>\"100ms\"</code>)</li> <li><code>s</code> - seconds (e.g., <code>\"5s\"</code> = 5000 milliseconds)</li> <li><code>m</code> - minutes (e.g., <code>\"5m\"</code> = 300000 milliseconds)</li> <li><code>h</code> - hours (e.g., <code>\"1h\"</code> = 3600000 milliseconds)</li> <li><code>d</code> - days (e.g., <code>\"1d\"</code> = 86400000 milliseconds)</li> </ul> </li> </ul> <p>Examples: <pre><code>general:\n  # All these are equivalent (3 seconds):\n  # connect_timeout: 3000      # backward compatible (milliseconds)\n  # connect_timeout: \"3s\"      # human-readable\n  # connect_timeout: \"3000ms\"  # explicit milliseconds\n  connect_timeout: \"3s\"\n  idle_timeout: \"5m\"         # 5 minutes\n  server_lifetime: \"1h\"      # 1 hour\n</code></pre></p>"},{"location":"reference/general/#byte-size-format","title":"Byte Size Format","text":"<p>Byte size values can be specified as:</p> <ul> <li>Plain numbers: interpreted as bytes (e.g., <code>1048576</code> = 1 MB)</li> <li>String with suffix (case-insensitive):<ul> <li><code>B</code> - bytes (e.g., <code>\"1024B\"</code>)</li> <li><code>K</code> or <code>KB</code> - kilobytes (e.g., <code>\"1K\"</code> or <code>\"1KB\"</code> = 1024 bytes)</li> <li><code>M</code> or <code>MB</code> - megabytes (e.g., <code>\"1M\"</code> or <code>\"1MB\"</code> = 1048576 bytes)</li> <li><code>G</code> or <code>GB</code> - gigabytes (e.g., <code>\"1G\"</code> or <code>\"1GB\"</code> = 1073741824 bytes)</li> </ul> </li> </ul> <p>Note: Uses binary prefixes (1 KB = 1024 bytes, not 1000 bytes).</p> <p>Examples: <pre><code>general:\n  # All these are equivalent (256 MB):\n  # max_memory_usage: 268435456  # backward compatible (bytes)\n  # max_memory_usage: \"256MB\"    # human-readable\n  # max_memory_usage: \"256M\"     # short form\n  max_memory_usage: \"256MB\"\n  unix_socket_buffer_size: \"1MB\" # 1 MB\n  worker_stack_size: \"8MB\"       # 8 MB\n</code></pre></p>"},{"location":"reference/general/#general-settings","title":"General Settings","text":""},{"location":"reference/general/#host","title":"host","text":"<p>Listen host (TCP v4 only).</p> <p>Default: <code>\"0.0.0.0\"</code>.</p>"},{"location":"reference/general/#port","title":"port","text":"<p>Listen port for incoming connections.</p> <p>Default: <code>6432</code>.</p>"},{"location":"reference/general/#backlog","title":"backlog","text":"<p>TCP backlog for incoming connections. A value of zero sets the <code>max_connections</code> as value for the TCP backlog.</p> <p>Default: <code>0</code>.</p>"},{"location":"reference/general/#max_connections","title":"max_connections","text":"<p>The maximum number of clients that can connect to the pooler simultaneously. When this limit is reached: * A client connecting without SSL will receive the expected error (code: <code>53300</code>, message: <code>sorry, too many clients already</code>). * A client connecting via SSL will see a message indicating that the server does not support the SSL protocol.</p> <p>Default: <code>8192</code>.</p>"},{"location":"reference/general/#max_concurrent_creates","title":"max_concurrent_creates","text":"<p>Maximum number of server connections that can be created concurrently per pool. This setting uses a semaphore to limit parallel connection creation, which significantly improves performance during cold start and burst scenarios.</p> <p>Higher values allow faster pool warm-up but may increase load on the PostgreSQL server during connection storms. Lower values provide more gradual connection creation.</p> <p>Default: <code>4</code>.</p>"},{"location":"reference/general/#tls_mode","title":"tls_mode","text":"<p>The TLS mode for incoming connections. It can be one of the following:</p> <ul> <li><code>allow</code> - TLS connections are allowed but not required. The pg_doorman will attempt to establish a TLS connection if the client requests it.</li> <li><code>disable</code> - TLS connections are not allowed. All connections will be established without TLS encryption.</li> <li><code>require</code> - TLS connections are required. The pg_doorman will only accept connections that use TLS encryption.</li> <li><code>verify-full</code> - TLS connections are required and the pg_doorman will verify the client certificate. This mode provides the highest level of security.</li> </ul> <p>Default: <code>\"allow\"</code>.</p>"},{"location":"reference/general/#tls_ca_file","title":"tls_ca_file","text":"<p>The file containing the CA certificate to verify the client certificate. This is required when <code>tls_mode</code> is set to <code>verify-full</code>.</p> <p>Default: <code>None</code>.</p>"},{"location":"reference/general/#tls_private_key","title":"tls_private_key","text":"<p>The path to the private key file for TLS connections. This is required to enable TLS for incoming client connections. Must be used together with <code>tls_certificate</code>.</p> <p>Default: <code>None</code>.</p>"},{"location":"reference/general/#tls_certificate","title":"tls_certificate","text":"<p>The path to the certificate file for TLS connections. This is required to enable TLS for incoming client connections. Must be used together with <code>tls_private_key</code>.</p> <p>Default: <code>None</code>.</p>"},{"location":"reference/general/#tls_rate_limit_per_second","title":"tls_rate_limit_per_second","text":"<p>Limit the number of simultaneous attempts to create a TLS session. Any value other than zero implies that there is a queue through which clients must pass in order to establish a TLS connection. In some cases, this is necessary in order to launch an application that opens many connections at startup (the so-called \"hot start\").</p> <p>Default: <code>0</code>.</p>"},{"location":"reference/general/#daemon_pid_file","title":"daemon_pid_file","text":"<p>Enabling this setting enables daemon mode. Comment this out if you want to run pg_doorman in the foreground with <code>-d</code>.</p> <p>Default: <code>None</code>.</p>"},{"location":"reference/general/#syslog_prog_name","title":"syslog_prog_name","text":"<p>When specified, pg_doorman starts sending messages to syslog (using /dev/log or /var/run/syslog). Comment this out if you want to log to stdout.</p> <p>Default: <code>None</code>.</p>"},{"location":"reference/general/#log_client_connections","title":"log_client_connections","text":"<p>Log client connections for monitoring.</p> <p>Default: <code>true</code>.</p>"},{"location":"reference/general/#log_client_disconnections","title":"log_client_disconnections","text":"<p>Log client disconnections for monitoring.</p> <p>Default: <code>true</code>.</p>"},{"location":"reference/general/#worker_threads","title":"worker_threads","text":"<p>The number of worker processes (posix threads) that async serve clients, which affects the performance of pg_doorman. The more workers there are, the faster the system works, but only up to a certain limit (cpu count).</p> <p>This parameter also controls the number of shards in internal concurrent hash maps (DashMap). The shard count is calculated as <code>worker_threads * 4</code> rounded up to the nearest power of 2 (minimum 4 shards). This is important for Kubernetes deployments where CPU count detection may be incorrect, causing unnecessary overhead.</p> <p>Default: <code>4</code>.</p>"},{"location":"reference/general/#worker_cpu_affinity_pinning","title":"worker_cpu_affinity_pinning","text":"<p>Automatically assign workers to different CPUs (man 3 cpu_set).</p> <p>Default: <code>false</code>.</p>"},{"location":"reference/general/#tokio_global_queue_interval","title":"tokio_global_queue_interval","text":"<p>Tokio runtime settings. Controls how often the scheduler checks the global task queue. Modern tokio versions handle this well by default, so this parameter is optional.</p> <p>Default: not set (uses tokio's default).</p>"},{"location":"reference/general/#tokio_event_interval","title":"tokio_event_interval","text":"<p>Tokio runtime settings. Controls how often the scheduler checks for external events (I/O, timers). Modern tokio versions handle this well by default, so this parameter is optional.</p> <p>Default: not set (uses tokio's default).</p>"},{"location":"reference/general/#worker_stack_size","title":"worker_stack_size","text":"<p>Tokio runtime settings. Sets the stack size for worker threads. Modern tokio versions handle this well by default, so this parameter is optional.</p> <p>Default: not set (uses tokio's default).</p>"},{"location":"reference/general/#max_blocking_threads","title":"max_blocking_threads","text":"<p>Tokio runtime settings. Sets the maximum number of threads for blocking operations. Modern tokio versions handle this well by default, so this parameter is optional.</p> <p>Default: not set (uses tokio's default).</p>"},{"location":"reference/general/#connect_timeout","title":"connect_timeout","text":"<p>Connection timeout to server in milliseconds.</p> <p>Default: <code>3000</code> (3 sec).</p>"},{"location":"reference/general/#query_wait_timeout","title":"query_wait_timeout","text":"<p>Maximum time to wait for a query to complete, in milliseconds.</p> <p>Default: <code>5000</code> (5 sec).</p>"},{"location":"reference/general/#idle_timeout","title":"idle_timeout","text":"<p>Server idle timeout in milliseconds.</p> <p>Default: <code>300000000</code> (5000 min).</p>"},{"location":"reference/general/#server_lifetime","title":"server_lifetime","text":"<p>Server lifetime in milliseconds.</p> <p>Default: <code>300000</code> (5 min).</p>"},{"location":"reference/general/#retain_connections_time","title":"retain_connections_time","text":"<p>Interval for checking and closing idle connections that exceed <code>idle_timeout</code> or <code>server_lifetime</code>. The retain task runs periodically at this interval to clean up expired connections.</p> <p>Default: <code>30000</code> (30 sec).</p>"},{"location":"reference/general/#retain_connections_max","title":"retain_connections_max","text":"<p>Maximum number of idle connections to close per retain cycle. When set to <code>0</code>, all idle connections that exceed <code>idle_timeout</code> or <code>server_lifetime</code> will be closed immediately. When set to a positive value, at most that many connections will be closed per cycle across all pools.</p> <p>This parameter controls how aggressively pg_doorman closes idle connections. With the default value of <code>0</code> (unlimited), all expired connections are closed in each retain cycle, ensuring quick cleanup. If you need to limit the rate of connection closures (e.g., to reduce load spikes), set this to a positive value.</p> <p>Default: <code>0</code> (unlimited).</p>"},{"location":"reference/general/#server_idle_check_timeout","title":"server_idle_check_timeout","text":"<p>Time after which an idle server connection should be checked before being given to a client. This helps detect dead connections caused by PostgreSQL restart, network issues, or server-side idle timeouts.</p> <p>When a connection has been idle in the pool longer than this timeout, pg_doorman will send a minimal query (<code>;</code>) to verify the connection is still alive before returning it to the client. If the check fails, the connection is discarded and a new one is obtained.</p> <p>Set to <code>0</code> to disable the check (not recommended for production environments with potential network instability or PostgreSQL restarts).</p> <p>Default: <code>30s</code> (30 seconds).</p>"},{"location":"reference/general/#server_round_robin","title":"server_round_robin","text":"<p>In transactional pool mode, we can choose whether the last free server backend will be used or the next one will be selected. By default, the LRU (Least Recently Used) method is used, which has a positive impact on performance.</p> <p>Default: <code>false</code>.</p>"},{"location":"reference/general/#sync_server_parameters","title":"sync_server_parameters","text":"<p>If enabled, we strive to restore the parameters (via query <code>SET</code>) that were set by the client (and application_name) in transaction mode in other server backends. By default, this is disabled (false) due to performance. If you need to know <code>application_name</code>, but don't want to experience performance issues due to constant server queries <code>SET</code>, you can consider creating a separate pool for each application and using the <code>application_name</code> parameter in the <code>pool</code> settings.</p> <p>Default: <code>false</code>.</p>"},{"location":"reference/general/#tcp_so_linger","title":"tcp_so_linger","text":"<p>By default, pg_doorman send <code>RST</code> instead of keeping the connection open for a long time.</p> <p>Default: <code>0</code>.</p>"},{"location":"reference/general/#tcp_no_delay","title":"tcp_no_delay","text":"<p>TCP_NODELAY to disable Nagle's algorithm for lower latency.</p> <p>Default: <code>true</code>.</p>"},{"location":"reference/general/#tcp_keepalives_count","title":"tcp_keepalives_count","text":"<p>Keepalive enabled by default and overwrite OS defaults.</p> <p>Default: <code>5</code>.</p>"},{"location":"reference/general/#tcp_keepalives_idle","title":"tcp_keepalives_idle","text":"<p>Default: <code>5</code>.</p>"},{"location":"reference/general/#tcp_keepalives_interval","title":"tcp_keepalives_interval","text":"<p>Default: <code>1</code>.</p>"},{"location":"reference/general/#tcp_user_timeout","title":"tcp_user_timeout","text":"<p>Sets the <code>TCP_USER_TIMEOUT</code> socket option for client connections (in seconds). This option specifies the maximum time that transmitted data may remain unacknowledged before TCP will forcibly close the connection. This helps detect dead client connections faster than keepalive probes when the connection is actively sending data but the remote end has become unreachable (e.g., network failure, client crash).</p> <p>When set to a non-zero value, if data remains unacknowledged for this duration, the connection will be terminated. This is particularly useful to avoid 15-16 minute delays caused by TCP retransmission timeout when keepalive cannot help (e.g., during active data transmission).</p> <p>Note: This option is only supported on Linux. On other operating systems, this setting is ignored.</p> <p>Set to <code>0</code> to disable (use OS default).</p> <p>Default: <code>0</code> (disabled).</p>"},{"location":"reference/general/#unix_socket_buffer_size","title":"unix_socket_buffer_size","text":"<p>Buffer size for read and write operations when connecting to PostgreSQL via a unix socket.</p> <p>Default: <code>1048576</code>.</p>"},{"location":"reference/general/#admin_username","title":"admin_username","text":"<p>Access to the virtual admin database is carried out through the administrator's username and password.</p> <p>Default: <code>\"admin\"</code>.</p>"},{"location":"reference/general/#admin_password","title":"admin_password","text":"<p>Access to the virtual admin database is carried out through the administrator's username and password. It should be replaced with your secret.</p> <p>Default: <code>\"admin\"</code>.</p>"},{"location":"reference/general/#prepared_statements","title":"prepared_statements","text":"<p>Switcher to enable/disable caching of prepared statements.</p> <p>Default: <code>true</code>.</p>"},{"location":"reference/general/#prepared_statements_cache_size","title":"prepared_statements_cache_size","text":"<p>Cache size of prepared statements at the pool level (shared across all clients connecting to the same pool).  This cache stores the mapping from query hash to rewritten prepared statement name.</p> <p>Default: <code>8192</code>.</p>"},{"location":"reference/general/#client_prepared_statements_cache_size","title":"client_prepared_statements_cache_size","text":"<p>Maximum number of prepared statements cached per client connection. This is a protection mechanism against  malicious or misbehaving clients that don't call <code>DEALLOCATE</code> and could cause memory exhaustion by creating  unlimited prepared statements over long-running connections.</p> <p>When the limit is reached, the oldest entry is evicted from the client's cache. The evicted statement  can still be re-used later because the pool-level cache (<code>prepared_statements_cache_size</code>) retains the  query-to-server-name mapping.</p> <p>Set to <code>0</code> to disable the limit (unlimited cache size, relies on client calling <code>DEALLOCATE</code>).</p> <p>Default: <code>0</code> (unlimited).</p>"},{"location":"reference/general/#message_size_to_be_stream","title":"message_size_to_be_stream","text":"<p>Data responses from the server (message type 'D') greater than this value will be transmitted through the proxy in small chunks (1 MB).</p> <p>Default: <code>1048576</code>.</p>"},{"location":"reference/general/#max_memory_usage","title":"max_memory_usage","text":"<p>We calculate the total amount of memory used by the internal buffers for all current queries. If the limit is reached, the client will receive an error (256 MB).</p> <p>Default: <code>268435456</code>.</p>"},{"location":"reference/general/#shutdown_timeout","title":"shutdown_timeout","text":"<p>With a graceful shutdown, we wait for transactions to be completed within this time limit (10 seconds).</p> <p>Default: <code>10000</code>.</p>"},{"location":"reference/general/#proxy_copy_data_timeout","title":"proxy_copy_data_timeout","text":"<p>Maximum time to wait for data copy operations during proxying, in milliseconds.</p> <p>Default: <code>15000</code> (15 sec).</p>"},{"location":"reference/general/#server_tls","title":"server_tls","text":"<p>Enable TLS for connections to the PostgreSQL server. When enabled, pg_doorman will attempt to establish TLS connections to the backend PostgreSQL servers.</p> <p>Default: <code>false</code>.</p>"},{"location":"reference/general/#verify_server_certificate","title":"verify_server_certificate","text":"<p>Verify the PostgreSQL server's TLS certificate when connecting with TLS. This setting is only relevant when <code>server_tls</code> is enabled.</p> <p>Default: <code>false</code>.</p>"},{"location":"reference/general/#hba","title":"hba","text":"<p>The list of IP addresses from which it is permitted to connect to the pg-doorman.</p>"},{"location":"reference/general/#pg_hba","title":"pg_hba","text":"<p>New-style client access control in native PostgreSQL <code>pg_hba.conf</code> format. This allows you to define fine-grained access rules similar to PostgreSQL, including per-database, per-user, address ranges, and TLS requirements.</p> <p>You can specify <code>general.pg_hba</code> in three ways:</p> <ul> <li>As a multi-line string with the contents of a <code>pg_hba.conf</code> file</li> <li>As an object with <code>path</code> that points to a file on disk</li> <li>As an object with <code>content</code> containing the rules as a string</li> </ul> <p>Examples:</p> <pre><code>[general]\n# Inline content (triple-quoted TOML string)\npg_hba = \"\"\"\n# type   database  user   address         method\nhost     all       all    10.0.0.0/8      md5\nhostssl  all       all    0.0.0.0/0       scram-sha-256\nhostnossl all      all    192.168.1.0/24  trust\n\"\"\"\n\n# Or load from file\n# pg_hba = { path = \"./pg_hba.conf\" }\n\n# Or embed as a single-line string\n# pg_hba = { content = \"host all all 127.0.0.1/32 trust\" }\n</code></pre> <p>Supported fields and methods: - Connection types: <code>local</code>, <code>host</code>, <code>hostssl</code>, <code>hostnossl</code> (TLS-aware matching is honored) - Database matcher: a name or <code>all</code> - User matcher: a name or <code>all</code> - Address: CIDR form like <code>1.2.3.4/32</code> or <code>::1/128</code> (required for non-<code>local</code> rules) - Methods: <code>trust</code>, <code>md5</code>, <code>scram-sha-256</code> (unknown methods are parsed but treated as not-allowed by the checker)</p> <p>Precedence and compatibility: - <code>general.pg_hba</code> supersedes the legacy <code>general.hba</code> list. You cannot set both at the same time; configuration validation will reject this combination. - Rules are evaluated in order; the first matching rule decides the outcome.</p> <p>Behavior of method = trust: - When a matching rule has <code>trust</code>, PgDoorman will accept the connection without requesting a password. This mirrors PostgreSQL behavior. - Specifically, if <code>trust</code> matches, PgDoorman will skip password verification even if the user has an <code>md5</code> or <code>scram-sha-256</code> password stored. This affects both MD5 and SCRAM flows. - TLS constraints from the rule are respected: <code>hostssl</code> requires TLS, <code>hostnossl</code> forbids TLS.</p> <p>Admin console access: - <code>general.pg_hba</code> rules apply to the special admin database <code>pgdoorman</code> as well. - This means you can allow admin access with the <code>trust</code> method when a matching rule is present, for example:   <pre><code>host  pgdoorman  admin  127.0.0.1/32  trust\n</code></pre></p> <p>Notes and limitations: - Only a minimal subset of <code>pg_hba.conf</code> is supported that is sufficient for most proxy use-cases (type, database, user, address, method). Additional options (like <code>clientcert</code>) are currently ignored. - For authentication methods other than <code>trust</code>, PgDoorman performs the corresponding challenge/response with the client. - For Talos/JWT/PAM flows configured at the pool/user level, <code>trust</code> still bypasses the client password prompt; however, those modes may be used when <code>trust</code> does not match.</p>"},{"location":"reference/general/#pooler_check_query","title":"pooler_check_query","text":"<p>This query will not be sent to the server if it is run as a SimpleQuery. It can be used to check the connection at the application level.</p> <p>Default: <code>;</code>.</p>"},{"location":"reference/pool/","title":"Pool Settings","text":""},{"location":"reference/pool/#pool-settings","title":"Pool Settings","text":"<p>Each record in the pool is the name of the virtual database that the pg-doorman client can connect to.</p> <pre><code>[pools.exampledb] # Declaring the 'exampledb' database\n</code></pre>"},{"location":"reference/pool/#server_host","title":"server_host","text":"<p>The directory with unix sockets or the IPv4 address of the PostgreSQL server that serves this pool.</p> <p>Example: <code>\"/var/run/postgresql\"</code> or <code>\"127.0.0.1\"</code>.</p>"},{"location":"reference/pool/#server_port","title":"server_port","text":"<p>The port through which PostgreSQL server accepts incoming connections.</p> <p>Default: <code>5432</code>.</p>"},{"location":"reference/pool/#server_database","title":"server_database","text":"<p>Optional parameter that determines which database should be connected to on the PostgreSQL server.</p> <p>Example: <code>\"exampledb-2\"</code></p>"},{"location":"reference/pool/#application_name","title":"application_name","text":"<p>Parameter application_name, is sent to the server when opening a connection with PostgreSQL. It may be useful with the sync_server_parameters = false setting.</p> <p>Example: <code>\"exampledb-pool\"</code></p>"},{"location":"reference/pool/#connect_timeout","title":"connect_timeout","text":"<p>Maximum time to allow for establishing a new server connection for this pool, in milliseconds. If not specified, the global connect_timeout setting is used.</p> <p>Default: <code>None</code> (uses global setting).</p>"},{"location":"reference/pool/#idle_timeout","title":"idle_timeout","text":"<p>Close idle connections in this pool that have been opened for longer than this value, in milliseconds. If not specified, the global idle_timeout setting is used.</p> <p>Default: <code>None</code> (uses global setting).</p>"},{"location":"reference/pool/#server_lifetime","title":"server_lifetime","text":"<p>Close server connections in this pool that have been opened for longer than this value, in milliseconds. Only applied to idle connections. If not specified, the global server_lifetime setting is used.</p> <p>Default: <code>None</code> (uses global setting).</p>"},{"location":"reference/pool/#pool_mode","title":"pool_mode","text":"<ul> <li> <p><code>session</code> :   Server is released back to pool after client disconnects.</p> </li> <li> <p><code>transaction</code> :   Server is released back to pool after transaction finishes.</p> </li> </ul> <p>Example: <code>\"session\"</code> or <code>\"transaction\"</code>.</p>"},{"location":"reference/pool/#log_client_parameter_status_changes","title":"log_client_parameter_status_changes","text":"<p>Log information about any SET command in the log.</p> <p>Default: <code>false</code>.</p>"},{"location":"reference/pool/#cleanup_server_connections","title":"cleanup_server_connections","text":"<p>When enabled, the pool will automatically clean up server connections that are no longer needed. This helps manage resources efficiently by closing idle connections.</p> <p>Default: <code>true</code>.</p>"},{"location":"reference/pool/#pool-users-settings","title":"Pool Users Settings","text":"<pre><code>[pools.exampledb.users.0]\nusername = \"exampledb-user-0\" # A virtual user who can connect to this virtual database.\n</code></pre>"},{"location":"reference/pool/#username","title":"username","text":"<p>A virtual username who can connect to this virtual database (pool).</p> <p>Example: <code>\"exampledb-user-0\"</code>.</p>"},{"location":"reference/pool/#password","title":"password","text":"<p>The password for the virtual pool user. Password can be specified in <code>MD5</code>, <code>SCRAM-SHA-256</code>, or <code>JWT</code> format. Also, you can create a mirror list of users using secrets from the PostgreSQL instance: <code>select usename, passwd from pg_shadow</code>.</p> <p>Example: <code>md5dd9a0f2...76a09bbfad</code> or <code>SCRAM-SHA-256$4096:E+QNCSW3r58yM+Twj1P5Uw==$LQrKl...Ro1iBKM=</code> or in jwt format: <code>jwt-pkey-fpath:/etc/pg_doorman/jwt/public-exampledb-user.pem</code></p>"},{"location":"reference/pool/#auth_pam_service","title":"auth_pam_service","text":"<p>The pam-service that is responsible for client authorization. In this case, pg_doorman will ignore the <code>password</code> value.</p>"},{"location":"reference/pool/#server_username","title":"server_username","text":"<p>The real server user of the database who connects to this database.</p> <p>Example: <code>\"exampledb_server_user\"</code>.</p>"},{"location":"reference/pool/#server_password","title":"server_password","text":"<p>The password (plain text) of real server user of the database who connects to this database.</p> <p>Example: <code>\"password\"</code>.</p>"},{"location":"reference/pool/#pool_size","title":"pool_size","text":"<p>The maximum number of simultaneous connections to the PostgreSQL server available for this pool and user.</p> <p>Default: <code>40</code>.</p>"},{"location":"reference/pool/#min_pool_size","title":"min_pool_size","text":"<p>The minimum number of connections to maintain in the pool for this user. This helps with performance by keeping connections ready. If specified, it must be less than or equal to pool_size.</p> <p>Default: <code>None</code>.</p>"},{"location":"reference/pool/#server_lifetime_1","title":"server_lifetime","text":"<p>Close server connections for this user that have been opened for longer than this value, in milliseconds. Only applied to idle connections. If not specified, the pool's server_lifetime setting is used.</p> <p>Default: <code>None</code> (uses pool setting).</p>"},{"location":"reference/prometheus/","title":"Prometheus Settings","text":"<p>pg_doorman includes a Prometheus metrics exporter that provides detailed insights into the performance and behavior of your connection pools. This document describes how to enable and use the Prometheus metrics exporter, as well as the available metrics.</p>"},{"location":"reference/prometheus/#enabling-prometheus-metrics","title":"Enabling Prometheus Metrics","text":"<p>To enable the Prometheus metrics exporter, add the following configuration to your <code>pg_doorman.toml</code> file:</p> <pre><code>[prometheus]\nenabled = true\nhost = \"0.0.0.0\"  # The host on which the metrics server will listen\nport = 9127       # The port on which the metrics server will listen\n</code></pre>"},{"location":"reference/prometheus/#configuration-options","title":"Configuration Options","text":"Option Description Default <code>enabled</code> Enable or disable the Prometheus metrics exporter <code>false</code> <code>host</code> The host on which the Prometheus metrics exporter will listen <code>\"0.0.0.0\"</code> <code>port</code> The port on which the Prometheus metrics exporter will listen <code>9127</code>"},{"location":"reference/prometheus/#configuring-prometheus","title":"Configuring Prometheus","text":"<p>Add the following job to your Prometheus configuration to scrape metrics from pg_doorman:</p> <pre><code>scrape_configs:\n  - job_name: 'pg_doorman'\n    static_configs:\n      - targets: ['&lt;pg_doorman_host&gt;:9127']\n</code></pre> <p>Replace <code>&lt;pg_doorman_host&gt;</code> with the hostname or IP address of your pg_doorman instance.</p>"},{"location":"reference/prometheus/#available-metrics","title":"Available Metrics","text":"<p>pg_doorman exposes the following metrics:</p>"},{"location":"reference/prometheus/#system-metrics","title":"System Metrics","text":"Metric Description <code>pg_doorman_total_memory</code> Total memory allocated to the pg_doorman process in bytes. Monitors the memory footprint of the application."},{"location":"reference/prometheus/#connection-metrics","title":"Connection Metrics","text":"Metric Description <code>pg_doorman_connection_count</code> Counter of new connections by type handled by pg_doorman. Types include: 'plain' (unencrypted connections), 'tls' (encrypted connections), 'cancel' (connection cancellation requests), and 'total' (sum of all connections)."},{"location":"reference/prometheus/#socket-metrics-linux-only","title":"Socket Metrics (Linux only)","text":"Metric Description <code>pg_doorman_sockets</code> Counter of sockets used by pg_doorman by socket type. Types include: 'tcp' (IPv4 TCP sockets), 'tcp6' (IPv6 TCP sockets), 'unix' (Unix domain sockets), and 'unknown' (sockets of unrecognized type). Only available on Linux systems."},{"location":"reference/prometheus/#pool-metrics","title":"Pool Metrics","text":"Metric Description <code>pg_doorman_pools_clients</code> Number of clients in connection pools by status, user, and database. Status values include: 'idle' (connected but not executing queries), 'waiting' (waiting for a server connection), and 'active' (currently executing queries). Helps monitor connection pool utilization and client distribution. <code>pg_doorman_pools_servers</code> Number of servers in connection pools by status, user, and database. Status values include: 'active' (actively serving clients) and 'idle' (available for new connections). Helps monitor server availability and load distribution. <code>pg_doorman_pools_bytes</code> Total bytes transferred through connection pools by direction, user, and database. Direction values include: 'received' (bytes received from clients) and 'sent' (bytes sent to clients). Useful for monitoring network traffic and identifying high-volume connections."},{"location":"reference/prometheus/#query-and-transaction-metrics","title":"Query and Transaction Metrics","text":"Metric Description <code>pg_doorman_pools_queries_percentile</code> Query execution time percentiles by user and database. Percentile values include: '99', '95', '90', and '50' (median). Values are in milliseconds. Helps identify slow queries and performance trends across different users and databases. <code>pg_doorman_pools_transactions_percentile</code> Transaction execution time percentiles by user and database. Percentile values include: '99', '95', '90', and '50' (median). Values are in milliseconds. Helps monitor transaction performance and identify long-running transactions that might impact database performance. <code>pg_doorman_pools_transactions_count</code> Counter of transactions executed in connection pools by user and database. Helps track transaction volume and identify users or databases with high transaction rates. <code>pg_doorman_pools_transactions_total_time</code> Total time spent executing transactions in connection pools by user and database. Values are in milliseconds. Helps monitor overall transaction performance and identify users or databases with high transaction execution times. <code>pg_doorman_pools_queries_count</code> Counter of queries executed in connection pools by user and database. Helps track query volume and identify users or databases with high query rates. <code>pg_doorman_pools_queries_total_time</code> Total time spent executing queries in connection pools by user and database. Values are in milliseconds. Helps monitor overall query performance and identify users or databases with high query execution times. <code>pg_doorman_pools_avg_wait_time</code> Average wait time for clients in connection pools by user and database. Values are in milliseconds. Helps monitor client wait times and identify potential bottlenecks."},{"location":"reference/prometheus/#server-metrics","title":"Server Metrics","text":"Metric Description <code>pg_doorman_servers_prepared_hits</code> Counter of prepared statement hits in databases backends by user and database. Helps track the effectiveness of prepared statements in reducing query parsing overhead. <code>pg_doorman_servers_prepared_misses</code> Counter of prepared statement misses in databases backends by user and database. Helps identify queries that could benefit from being prepared to improve performance."},{"location":"reference/prometheus/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>You can create a Grafana dashboard to visualize these metrics. Here's a simple example of panels you might want to include:</p> <ol> <li>Connection counts by type</li> <li>Memory usage over time</li> <li>Client and server counts by pool</li> <li>Query and transaction performance percentiles</li> <li>Network traffic by pool</li> </ol>"},{"location":"reference/prometheus/#example-queries","title":"Example Queries","text":"<p>Here are some example Prometheus queries that you might find useful:</p>"},{"location":"reference/prometheus/#connection-rate","title":"Connection Rate","text":"<pre><code>rate(pg_doorman_connection_count{type=\"total\"}[5m])\n</code></pre>"},{"location":"reference/prometheus/#pool-utilization","title":"Pool Utilization","text":"<pre><code>sum by (database) (pg_doorman_pools_clients{status=\"active\"}) / sum by (database) (pg_doorman_pools_servers{status=\"active\"} + pg_doorman_pools_servers{status=\"idle\"})\n</code></pre>"},{"location":"reference/prometheus/#slow-queries","title":"Slow Queries","text":"<pre><code>pg_doorman_pools_queries_percentile{percentile=\"99\"}\n</code></pre>"},{"location":"reference/prometheus/#client-wait-time","title":"Client Wait Time","text":"<pre><code>pg_doorman_pools_avg_wait_time\n</code></pre>"},{"location":"tutorials/basic-usage/","title":"PgDoorman Basic Usage Guide","text":"<p>PgDoorman is a high-performance PostgreSQL connection pooler based on PgCat. This comprehensive guide will help you get started with configuring, running, and managing PgDoorman for your PostgreSQL environment.</p>"},{"location":"tutorials/basic-usage/#command-line-options","title":"Command Line Options","text":"<p>PgDoorman offers several command-line options to customize its behavior when starting the service:</p> <pre><code>$ pg_doorman --help\n\nPgDoorman: Nextgen PostgreSQL Pooler (based on PgCat)\n\nUsage: pg_doorman [OPTIONS] [CONFIG_FILE] [COMMAND]\n\nCommands:\n  generate  Generate configuration for pg_doorman by connecting to PostgreSQL and auto-detecting databases and users\n  help      Print this message or the help of the given subcommand(s)\n\nArguments:\n  [CONFIG_FILE]  [env: CONFIG_FILE=] [default: pg_doorman.toml]\n\nOptions:\n  -l, --log-level &lt;LOG_LEVEL&gt;    [env: LOG_LEVEL=] [default: INFO]\n  -F, --log-format &lt;LOG_FORMAT&gt;  [env: LOG_FORMAT=] [default: text] [possible values: text, structured, debug]\n  -n, --no-color                 disable colors in the log output [env: NO_COLOR=]\n  -d, --daemon                   run as daemon [env: DAEMON=]\n  -h, --help                     Print help\n  -V, --version                  Print version\n</code></pre>"},{"location":"tutorials/basic-usage/#available-options","title":"Available Options","text":"Option Description <code>-d</code>, <code>--daemon</code> Run in the background. Without this option, the process will run in the foreground.In daemon mode, setting <code>daemon_pid_file</code> and <code>syslog_prog_name</code> is required. No log messages will be written to stderr after going into the background. <code>-l</code>, <code>--log-level</code> Set log level: <code>INFO</code>, <code>DEBUG</code>, or <code>WARN</code>. <code>-F</code>, <code>--log-format</code> Set log format. Possible values: <code>text</code>, <code>structured</code>, <code>debug</code>. <code>-n</code>, <code>--no-color</code> Disable colors in the log output. <code>-V</code>, <code>--version</code> Show version information. <code>-h</code>, <code>--help</code> Show help information."},{"location":"tutorials/basic-usage/#setup-and-configuration","title":"Setup and Configuration","text":""},{"location":"tutorials/basic-usage/#configuration-file-structure","title":"Configuration File Structure","text":"<p>PgDoorman uses a TOML format configuration file to define its behavior. The configuration file is organized into several sections:</p> <ul> <li><code>[general]</code> - Global settings for the PgDoorman service</li> <li><code>[pools]</code> - Database pool definitions</li> <li><code>[pools.&lt;name&gt;]</code> - Settings for a specific database pool</li> <li><code>[pools.&lt;name&gt;.users.&lt;n&gt;]</code> - User settings for a specific database pool</li> </ul> <p>Important</p> <p>Some parameters must be specified in the configuration file for PgDoorman to start, even if they have default values. For example, you must specify an admin username and password to access the administrative console.</p>"},{"location":"tutorials/basic-usage/#minimal-configuration-example","title":"Minimal Configuration Example","text":"<p>Here's a minimal configuration example to get you started:</p> <pre><code># Global settings\n[general]\nhost = \"0.0.0.0\"    # Listen on all interfaces\nport = 6432         # Port for client connections\n\n# Admin credentials for the management console\nadmin_username = \"admin\"\nadmin_password = \"admin\"  # Change this in production!\n\n# Database pools section\n[pools]\n\n# Example database pool\n[pools.exampledb]\nserver_host = \"127.0.0.1\"  # PostgreSQL server address\nserver_port = 5432         # PostgreSQL server port\npool_mode = \"transaction\"  # Connection pooling mode\n\n# User configuration for this pool\n[pools.exampledb.users.0]\npool_size = 40             # Maximum number of connections in the pool\nusername = \"doorman\"       # Username for PostgreSQL server\npassword = \"SCRAM-SHA-256$4096:6nD+Ppi9rgaNyP7...MBiTld7xJipwG/X4=\"  # Hashed password\n</code></pre> <p>For a complete list of configuration options and their descriptions, see the Settings Reference Guide.</p>"},{"location":"tutorials/basic-usage/#automatic-configuration-generation","title":"Automatic Configuration Generation","text":"<p>PgDoorman provides a powerful <code>generate</code> command that can automatically create a configuration file by connecting to your PostgreSQL server and detecting databases and users:</p> <pre><code># View all available options\npg_doorman generate --help\n\n# Generate a configuration file with default settings\npg_doorman generate --output pg_doorman.toml\n</code></pre> <p>The <code>generate</code> command supports several options:</p> Option Description <code>--host</code> PostgreSQL host to connect to (uses localhost if not specified) <code>--port</code>, <code>-p</code> PostgreSQL port to connect to (default: 5432) <code>--user</code>, <code>-u</code> PostgreSQL user to connect as (requires superuser privileges to read pg_shadow) <code>--password</code> PostgreSQL password to connect with <code>--database</code>, <code>-d</code> PostgreSQL database to connect to (uses same name as user if not specified) <code>--ssl</code> PostgreSQL connection to server via SSL/TLS <code>--pool-size</code> Pool size for the generated configuration (default: 40) <code>--session-pool-mode</code>, <code>-s</code> Session pool mode for the generated configuration <code>--output</code>, <code>-o</code> Output file for the generated configuration (uses stdout if not specified) <code>--server-host</code> Override server_host in config (uses the host parameter if not specified) <p>The command connects to your PostgreSQL server, automatically detects all databases and users, and creates a complete configuration file with appropriate settings. This is especially useful for quickly setting up PgDoorman in new environments or when you have many databases and users to configure.</p> <p>PostgreSQL Environment Variables</p> <p>The <code>generate</code> command also respects standard PostgreSQL environment variables like <code>PGHOST</code>, <code>PGPORT</code>, <code>PGUSER</code>, <code>PGPASSWORD</code>, and <code>PGDATABASE</code>.</p> <p>Authentication Required</p> <p>If your PostgreSQL server requires authentication in pg_hba.conf, you will need to manually set the <code>server_password</code> parameter in the configuration file after using the <code>generate</code> command.</p> <p>Superuser Privileges</p> <p>Reading user information from PostgreSQL requires superuser privileges to access the <code>pg_shadow</code> table.</p>"},{"location":"tutorials/basic-usage/#client-access-control-pg_hba","title":"Client access control (pg_hba)","text":"<p>PgDoorman can enforce client access rules using PostgreSQL-style <code>pg_hba.conf</code> semantics via the <code>general.pg_hba</code> parameter. You can embed rules directly in the config or reference a file path. See the reference section for full examples.</p> <p>Trust mode: when a matching rule uses <code>trust</code>, PgDoorman will accept connections without prompting the client for a password, mirroring PostgreSQL behavior. TLS-related rule types are honored: <code>hostssl</code> requires TLS, <code>hostnossl</code> forbids TLS.</p>"},{"location":"tutorials/basic-usage/#running-pgdoorman","title":"Running PgDoorman","text":"<p>After creating your configuration file, you can run PgDoorman from the command line:</p> <pre><code>$ pg_doorman pg_doorman.toml\n</code></pre> <p>If you don't specify a configuration file, PgDoorman will look for <code>pg_doorman.toml</code> in the current directory.</p>"},{"location":"tutorials/basic-usage/#connecting-to-postgresql-via-pgdoorman","title":"Connecting to PostgreSQL via PgDoorman","text":"<p>Once PgDoorman is running, connect to it instead of connecting directly to your PostgreSQL database:</p> <pre><code>$ psql -h localhost -p 6432 -U doorman exampledb\n</code></pre> <p>Your application's connection string should be updated to point to PgDoorman instead of directly to PostgreSQL:</p> <pre><code>postgresql://doorman:password@localhost:6432/exampledb\n</code></pre> <p>PgDoorman will handle the connection pooling transparently, so your application doesn't need to be aware that it's connecting through a pooler.</p>"},{"location":"tutorials/basic-usage/#administration","title":"Administration","text":""},{"location":"tutorials/basic-usage/#admin-console","title":"Admin Console","text":"<p>PgDoorman provides a powerful administrative interface that allows you to monitor and manage the connection pooler. You can access this interface by connecting to the special administration database named pgdoorman:</p> <pre><code>$ psql -h localhost -p 6432 -U admin pgdoorman\n</code></pre> <p>Once connected, you can view available commands:</p> <pre><code>pgdoorman=&gt; SHOW HELP;\nNOTICE:  Console usage\nDETAIL:\n    SHOW HELP|CONFIG|DATABASES|POOLS|POOLS_EXTENDED|CLIENTS|SERVERS|USERS|VERSION\n    SHOW LISTS\n    SHOW CONNECTIONS\n    SHOW STATS\n    RELOAD\n    SHUTDOWN\n    SHOW\n</code></pre> <p>Protocol Compatibility</p> <p>The admin console currently supports only the simple query protocol. Some database drivers use the extended query protocol for all commands, making them unsuitable for admin console access. In such cases, use the <code>psql</code> command-line client for administration.</p> <p>Security</p> <p>Only the user specified by <code>admin_username</code> in the configuration file is allowed to log in to the admin console. If your <code>general.pg_hba</code> rules allow it, the admin console can also be accessed using the <code>trust</code> method (no password prompt), for example:</p> <pre><code># Allow only local admin to access the admin DB without a password\nhost  pgdoorman  admin  127.0.0.1/32  trust\n</code></pre> <p>Use <code>trust</code> with extreme caution. Always restrict it by address and, where possible, require TLS via <code>hostssl</code>. In production, prefer password-based methods unless you fully understand the implications.</p>"},{"location":"tutorials/basic-usage/#monitoring-pgdoorman","title":"Monitoring PgDoorman","text":"<p>The admin console provides several commands to monitor the current state of PgDoorman:</p> <ul> <li><code>SHOW STATS</code> - View performance statistics</li> <li><code>SHOW CLIENTS</code> - List current client connections</li> <li><code>SHOW SERVERS</code> - List current server connections</li> <li><code>SHOW POOLS</code> - View connection pool status</li> <li><code>SHOW DATABASES</code> - List configured databases</li> <li><code>SHOW USERS</code> - List configured users</li> </ul> <p>These commands are described in detail in the Admin Console Commands section below.</p>"},{"location":"tutorials/basic-usage/#reloading-configuration","title":"Reloading Configuration","text":"<p>If you make changes to the <code>pg_doorman.toml</code> file, you can apply them without restarting the service:</p> <pre><code>pgdoorman=# RELOAD;\n</code></pre> <p>When you reload the configuration:</p> <ol> <li>PgDoorman reads the updated configuration file</li> <li>Changes to database connection parameters are detected</li> <li>Existing server connections are closed when they're next released (according to the pooling mode)</li> <li>New server connections immediately use the updated parameters</li> </ol> <p>This allows you to make configuration changes with minimal disruption to your applications.</p>"},{"location":"tutorials/basic-usage/#admin-console-commands","title":"Admin Console Commands","text":"<p>The admin console provides a set of commands to monitor and manage PgDoorman. These commands follow a SQL-like syntax and can be executed from any PostgreSQL client connected to the admin console.</p>"},{"location":"tutorials/basic-usage/#show-commands","title":"Show Commands","text":"<p>The <code>SHOW</code> commands display information about PgDoorman's operation. Each command provides different insights into the pooler's performance and current state.</p>"},{"location":"tutorials/basic-usage/#show-stats","title":"SHOW STATS","text":"<p>The <code>SHOW STATS</code> command displays comprehensive statistics about PgDoorman's operation:</p> <pre><code>pgdoorman=&gt; SHOW STATS;\n</code></pre> <p>Statistics are presented per database with the following metrics:</p> Metric Description <code>database</code> The database name these statistics apply to <code>total_xact_count</code> Total number of SQL transactions processed since startup <code>total_query_count</code> Total number of SQL commands processed since startup <code>total_received</code> Total bytes of network traffic received from clients <code>total_sent</code> Total bytes of network traffic sent to clients <code>total_xact_time</code> Total microseconds spent in transactions (including idle in transaction) <code>total_query_time</code> Total microseconds spent actively executing queries <code>total_wait_time</code> Total microseconds clients spent waiting for a server connection <code>avg_xact_count</code> Average transactions per second in the last 15-second period <code>avg_query_count</code> Average queries per second in the last 15-second period <code>avg_server_assignment_count</code> Average server assignments per second in the last 15-second period <code>avg_recv</code> Average bytes received per second from clients <code>avg_sent</code> Average bytes sent per second to clients <code>avg_xact_time</code> Average transaction duration in microseconds <code>avg_query_time</code> Average query duration in microseconds <code>avg_wait_time</code> Average time clients spent waiting for a server in microseconds <p>Performance Monitoring</p> <p>Pay special attention to the <code>avg_wait_time</code> metric. If this value is consistently high, it may indicate that your pool size is too small for your workload.</p>"},{"location":"tutorials/basic-usage/#show-servers","title":"SHOW SERVERS","text":"<p>The <code>SHOW SERVERS</code> command displays detailed information about all server connections:</p> <pre><code>pgdoorman=&gt; SHOW SERVERS;\n</code></pre> Column Description <code>server_id</code> Unique identifier for the server connection <code>server_process_id</code> PID of the backend PostgreSQL server process (if available) <code>database_name</code> Name of the database this connection is using <code>user</code> Username PgDoorman uses to connect to the PostgreSQL server <code>application_name</code> Value of the <code>application_name</code> parameter set on the server connection <code>state</code> Current state of the connection: active, idle, or used <code>wait</code> Wait state of the connection: idle, read, or write <code>transaction_count</code> Total number of transactions processed by this connection <code>query_count</code> Total number of queries processed by this connection <code>bytes_sent</code> Total bytes sent to the PostgreSQL server <code>bytes_received</code> Total bytes received from the PostgreSQL server <code>age_seconds</code> Lifetime of the current server connection in seconds <code>prepare_cache_hit</code> Number of prepared statement cache hits <code>prepare_cache_miss</code> Number of prepared statement cache misses <code>prepare_cache_size</code> Number of unique prepared statements in the cache <p>Connection States</p> <ul> <li>active: The connection is currently executing a query</li> <li>idle: The connection is available for use</li> <li>used: The connection is allocated to a client but not currently executing a query</li> </ul>"},{"location":"tutorials/basic-usage/#show-clients","title":"SHOW CLIENTS","text":"<p>The <code>SHOW CLIENTS</code> command displays information about all client connections to PgDoorman:</p> <pre><code>pgdoorman=&gt; SHOW CLIENTS;\n</code></pre> Column Description <code>client_id</code> Unique identifier for the client connection <code>database</code> Name of the database (pool) the client is connected to <code>user</code> Username the client used to connect <code>addr</code> Client's IP address and port (IP:port) <code>tls</code> Whether the connection uses TLS encryption (true or false) <code>state</code> Current state of the client connection: active, idle, or waiting <code>wait</code> Wait state of the client connection: idle, read, or write <code>transaction_count</code> Total number of transactions processed for this client <code>query_count</code> Total number of queries processed for this client <code>age_seconds</code> Lifetime of the client connection in seconds <p>Monitoring Long-Running Connections</p> <p>The <code>age_seconds</code> column can help identify long-running connections that might be holding resources unnecessarily. Consider implementing connection timeouts in your application for idle connections.</p>"},{"location":"tutorials/basic-usage/#show-pools","title":"SHOW POOLS","text":"<p>The <code>SHOW POOLS</code> command displays information about connection pools. A new pool entry is created for each (database, user) pair:</p> <pre><code>pgdoorman=&gt; SHOW POOLS;\n</code></pre> Column Description <code>database</code> Name of the database <code>user</code> Username associated with this pool <code>pool_mode</code> Pooling mode in use: session or transaction <code>cl_active</code> Number of active client connections (linked to servers or idle) <code>cl_waiting</code> Number of client connections waiting for a server connection <code>sv_active</code> Number of server connections linked to clients <code>sv_idle</code> Number of idle server connections available for immediate use <code>sv_login</code> Number of server connections currently in the login process <code>maxwait</code> Maximum wait time in seconds for the oldest client in the queue <code>maxwait_us</code> Microsecond part of the maximum waiting time <p>Performance Alert</p> <p>If the <code>maxwait</code> value starts increasing, your server pool may not be handling requests quickly enough. This could be due to an overloaded PostgreSQL server or insufficient <code>pool_size</code> setting.</p>"},{"location":"tutorials/basic-usage/#show-users","title":"SHOW USERS","text":"<p>The <code>SHOW USERS</code> command displays information about all configured users:</p> <pre><code>pgdoorman=&gt; SHOW USERS;\n</code></pre> Column Description <code>name</code> Username as configured in PgDoorman <code>pool_mode</code> Pooling mode assigned to this user: session or transaction"},{"location":"tutorials/basic-usage/#show-databases","title":"SHOW DATABASES","text":"<p>The <code>SHOW DATABASES</code> command displays information about all configured database pools:</p> <pre><code>pgdoorman=&gt; SHOW DATABASES;\n</code></pre> Column Description <code>database</code> Name of the configured database pool <code>host</code> Hostname of the PostgreSQL server PgDoorman connects to <code>port</code> Port number of the PostgreSQL server <code>pool_size</code> Maximum number of server connections for this database <code>min_pool_size</code> Minimum number of server connections to maintain <code>reserve_pool_size</code> Maximum number of additional connections allowed <code>pool_mode</code> Default pooling mode for this database <code>max_connections</code> Maximum allowed server connections (from <code>max_db_connections</code>) <code>current_connections</code> Current number of server connections for this database <p>Connection Management</p> <p>Monitor the ratio between <code>current_connections</code> and <code>pool_size</code> to ensure your pool is properly sized. If <code>current_connections</code> frequently reaches <code>pool_size</code>, consider increasing the pool size.</p>"},{"location":"tutorials/basic-usage/#show-sockets","title":"SHOW SOCKETS","text":"<p>The <code>SHOW SOCKETS</code> command displays low-level information about network sockets:</p> <pre><code>pgdoorman=&gt; SHOW SOCKETS;\n</code></pre> <p>This command includes all information shown in <code>SHOW CLIENTS</code> and <code>SHOW SERVERS</code> plus additional low-level details about the socket connections.</p>"},{"location":"tutorials/basic-usage/#show-version","title":"SHOW VERSION","text":"<p>The <code>SHOW VERSION</code> command displays the PgDoorman version information:</p> <pre><code>pgdoorman=&gt; SHOW VERSION;\n</code></pre> <p>This is useful for verifying which version you're running, especially after upgrades.</p>"},{"location":"tutorials/basic-usage/#control-commands","title":"Control Commands","text":"<p>PgDoorman provides control commands that allow you to manage the service operation directly from the admin console.</p>"},{"location":"tutorials/basic-usage/#shutdown","title":"SHUTDOWN","text":"<p>The <code>SHUTDOWN</code> command gracefully terminates the PgDoorman process:</p> <pre><code>pgdoorman=&gt; SHUTDOWN;\n</code></pre> <p>When executed:</p> <ol> <li>PgDoorman stops accepting new client connections</li> <li>Existing transactions are allowed to complete (within the configured timeout)</li> <li>All connections are closed</li> <li>The process exits</li> </ol> <p>Service Interruption</p> <p>Using the <code>SHUTDOWN</code> command will terminate the PgDoorman service, disconnecting all clients. Use this command with caution in production environments.</p>"},{"location":"tutorials/basic-usage/#reload","title":"RELOAD","text":"<p>The <code>RELOAD</code> command refreshes PgDoorman's configuration without restarting the service:</p> <pre><code>pgdoorman=&gt; RELOAD;\n</code></pre> <p>This command:</p> <ol> <li>Rereads the configuration file</li> <li>Updates all changeable settings</li> <li>Applies changes to connection parameters for new connections</li> <li>Maintains existing connections until they're released back to the pool</li> </ol> <p>Zero-Downtime Configuration Changes</p> <p>The <code>RELOAD</code> command allows you to modify most configuration parameters without disrupting existing connections. This is ideal for production environments where downtime must be minimized.</p>"},{"location":"tutorials/basic-usage/#signal-handling","title":"Signal Handling","text":"<p>PgDoorman responds to standard Unix signals for control and management. These signals can be sent using the <code>kill</code> command (e.g., <code>kill -HUP &lt;pid&gt;</code>).</p> Signal Description Effect SIGHUP Configuration reload Equivalent to the <code>RELOAD</code> command in the admin console. Rereads the configuration file and applies changes to settings. SIGTERM Immediate shutdown Forces PgDoorman to exit immediately. Active connections may be terminated abruptly. SIGINT Graceful shutdown Initiates a binary upgrade process. The current process starts a new instance and gracefully transfers connections. See Binary Upgrade Process for details. <p>Process Management</p> <p>In systemd-based environments, you can use <code>systemctl reload pg_doorman</code> to send SIGHUP and <code>systemctl restart pg_doorman</code> for a complete restart.</p>"},{"location":"tutorials/binary-upgrade/","title":"Binary Upgrade Process","text":""},{"location":"tutorials/binary-upgrade/#overview","title":"Overview","text":"<p>PgDoorman supports seamless binary upgrades that allow you to update the software with minimal disruption to your database connections. This document explains how the upgrade process works and what to expect during an upgrade.</p>"},{"location":"tutorials/binary-upgrade/#how-the-upgrade-process-works","title":"How the Upgrade Process Works","text":"<p>When you send a <code>SIGINT</code> signal to the PgDoorman process, the binary upgrade process is initiated:</p> <ol> <li>The current PgDoorman instance executes the exec command and starts a new, daemonized process</li> <li>The new process uses the <code>SO_REUSE_PORT</code> socket option, allowing the operating system to distribute incoming traffic to the new instance</li> <li>The old instance then closes its socket for incoming connections</li> <li>Existing connections are handled gracefully during the transition</li> </ol>"},{"location":"tutorials/binary-upgrade/#handling-existing-connections","title":"Handling Existing Connections","text":"<p>During the upgrade process, PgDoorman handles existing connections as follows:</p> <ol> <li>Current queries and transactions are allowed to complete within the specified <code>shutdown_timeout</code> (default: 10 seconds)</li> <li>After each query or transaction completes successfully, PgDoorman returns error code <code>58006</code> to the client</li> <li>This error code indicates to the client that they need to reconnect to the server</li> <li>After reconnecting, clients can safely retry their queries with the new PgDoorman instance</li> </ol>"},{"location":"tutorials/binary-upgrade/#important-considerations","title":"Important Considerations","text":"<p>Query Repetition</p> <p>Repeating a query without receiving error code <code>58006</code> may cause problems as described in this issue. Make sure your client application properly handles reconnection scenarios.</p> <p>Client Library Compatibility</p> <p>Be careful when using client libraries like <code>github.com/lib/pq</code> or Go's standard <code>database/sql</code> package. Ensure they properly handle the reconnection process during binary upgrades.</p>"},{"location":"tutorials/contributing/","title":"Contributing to PgDoorman","text":"<p>Thank you for your interest in contributing to PgDoorman! This guide will help you set up your development environment and understand the contribution process.</p>"},{"location":"tutorials/contributing/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/contributing/#prerequisites","title":"Prerequisites","text":"<p>For running integration tests, you only need:</p> <ul> <li>Docker (required)</li> <li>Make (required)</li> </ul> <p>Nix installation is NOT required \u2014 test environment reproducibility is ensured by Docker containers built with Nix.</p> <p>For local development (optional): - Rust (latest stable version) - Git</p>"},{"location":"tutorials/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork:    <pre><code>git clone https://github.com/YOUR-USERNAME/pg_doorman.git\ncd pg_doorman\n</code></pre></li> <li>Add the upstream repository:    <pre><code>git remote add upstream https://github.com/ozontech/pg_doorman.git\n</code></pre></li> </ol>"},{"location":"tutorials/contributing/#local-development","title":"Local Development","text":"<ol> <li> <p>Build the project:    <pre><code>cargo build\n</code></pre></p> </li> <li> <p>Build for performance testing:    <pre><code>cargo build --release\n</code></pre></p> </li> <li> <p>Configure PgDoorman:</p> </li> <li>Copy the example configuration: <code>cp pg_doorman.toml.example pg_doorman.toml</code></li> <li> <p>Adjust the configuration in <code>pg_doorman.toml</code> to match your setup</p> </li> <li> <p>Run PgDoorman:    <pre><code>cargo run --release\n</code></pre></p> </li> <li> <p>Run unit tests:    <pre><code>cargo test\n</code></pre></p> </li> </ol>"},{"location":"tutorials/contributing/#integration-testing","title":"Integration Testing","text":"<p>PgDoorman uses BDD (Behavior-Driven Development) tests with a Docker-based test environment. Reproducibility is guaranteed \u2014 all tests run inside Docker containers with identical environments.</p>"},{"location":"tutorials/contributing/#test-environment","title":"Test Environment","text":"<p>The test Docker image (built with Nix) includes: - PostgreSQL 16 - Go 1.24 - Python 3 with asyncpg, psycopg2, aiopg, pytest - Node.js 22 - .NET SDK 8 - Rust 1.87.0</p>"},{"location":"tutorials/contributing/#running-tests","title":"Running Tests","text":"<p>From the project root directory:</p> <pre><code># Pull the test image from registry\nmake pull\n\n# Or build locally (takes 10-15 minutes on first run)\nmake local-build\n\n# Run all BDD tests\nmake test-bdd\n\n# Run tests with specific tag\nmake test-bdd TAGS=@copy-protocol\nmake test-bdd TAGS=@cancel\nmake test-bdd TAGS=@admin-commands\n\n# Open interactive shell in test container\nmake shell\n</code></pre>"},{"location":"tutorials/contributing/#debug-mode","title":"Debug Mode","text":"<p>Enable debug output with the <code>DEBUG=1</code> environment variable:</p> <pre><code>DEBUG=1 make test-bdd TAGS=@copy-protocol\n</code></pre> <p>When <code>DEBUG=1</code> is set: - Tracing is enabled with DEBUG level - Thread IDs are shown in logs - Line numbers are included - PostgreSQL protocol details are visible - Detailed step-by-step execution is logged</p> <p>This is useful when: - Debugging failing tests - Understanding protocol-level communication - Investigating timing issues - Developing new test scenarios</p>"},{"location":"tutorials/contributing/#available-test-tags","title":"Available Test Tags","text":"Tag Description <code>@go</code> Go client tests <code>@python</code> Python client tests <code>@nodejs</code> Node.js client tests <code>@dotnet</code> .NET client tests <code>@rust</code> Rust protocol-level tests <code>@copy-protocol</code> COPY protocol tests <code>@cancel</code> Query cancellation tests <code>@admin-commands</code> Admin console commands <code>@admin-leak</code> Admin connection leak tests <code>@buffer-cleanup</code> Buffer cleanup tests <code>@rollback</code> Rollback functionality tests <code>@hba</code> HBA authentication tests <code>@prometheus</code> Prometheus metrics tests"},{"location":"tutorials/contributing/#writing-new-tests","title":"Writing New Tests","text":"<p>Tests are organized as BDD feature files in <code>tests/bdd/features/</code>. Each feature file describes test scenarios using Gherkin syntax.</p>"},{"location":"tutorials/contributing/#shell-tests-recommended-for-client-libraries","title":"Shell Tests (Recommended for Client Libraries)","text":"<p>Shell tests run external test commands (Go, Python, Node.js, etc.) and verify their output. This is the simplest way to test client library compatibility.</p> <p>Example (<code>tests/bdd/features/my-feature.feature</code>):</p> <pre><code>@go @mytag\nFeature: My feature description\n\n  Background:\n    Given PostgreSQL started with pg_hba.conf:\n      \"\"\"\n      local all all trust\n      host all all 127.0.0.1/32 trust\n      \"\"\"\n    And fixtures from \"tests/fixture.sql\" applied\n    And pg_doorman started with config:\n      \"\"\"\n      [general]\n      host = \"127.0.0.1\"\n      port = ${DOORMAN_PORT}\n      admin_username = \"admin\"\n      admin_password = \"admin\"\n\n      [pools.example_db]\n      server_host = \"127.0.0.1\"\n      server_port = ${PG_PORT}\n\n      [pools.example_db.users.0]\n      username = \"example_user_1\"\n      password = \"md58a67a0c805a5ee0384ea28e0dea557b6\"\n      pool_size = 40\n      \"\"\"\n\n  Scenario: Test my Go client\n    When I run shell command:\n      \"\"\"\n      export DATABASE_URL=\"postgresql://example_user_1:test@127.0.0.1:${DOORMAN_PORT}/example_db?sslmode=disable\"\n      cd tests/go &amp;&amp; go test -v -run TestMyTest ./mypackage\n      \"\"\"\n    Then the command should succeed\n    And the command output should contain \"PASS\"\n</code></pre> <p>Test implementation (in your preferred language): - Go: <code>tests/go/mypackage/my_test.go</code> - Python: <code>tests/python/test_my.py</code> - Node.js: <code>tests/nodejs/my.test.js</code> - .NET: <code>tests/dotnet/MyTest.cs</code></p>"},{"location":"tutorials/contributing/#rust-protocol-level-tests","title":"Rust Protocol-Level Tests","text":"<p>For testing PostgreSQL protocol behavior at the wire level, use Rust-based tests. These tests directly send and receive PostgreSQL protocol messages, allowing precise control and comparison.</p> <p>Example (<code>tests/bdd/features/protocol-test.feature</code>):</p> <pre><code>@rust @my-protocol-test\nFeature: Protocol behavior test\n  Testing that pg_doorman handles protocol messages identically to PostgreSQL\n\n  Background:\n    Given PostgreSQL started with pg_hba.conf:\n      \"\"\"\n      local all all trust\n      host all all 127.0.0.1/32 trust\n      \"\"\"\n    And fixtures from \"tests/fixture.sql\" applied\n    And pg_doorman started with config:\n      \"\"\"\n      [general]\n      host = \"127.0.0.1\"\n      port = ${DOORMAN_PORT}\n      admin_username = \"admin\"\n      admin_password = \"admin\"\n      pg_hba.content = \"host all all 127.0.0.1/32 trust\"\n\n      [pools.example_db]\n      server_host = \"127.0.0.1\"\n      server_port = ${PG_PORT}\n\n      [pools.example_db.users.0]\n      username = \"example_user_1\"\n      password = \"\"\n      pool_size = 10\n      \"\"\"\n\n  @my-scenario\n  Scenario: Query gives identical results from PostgreSQL and pg_doorman\n    When we login to postgres and pg_doorman as \"example_user_1\" with password \"\" and database \"example_db\"\n    And we send SimpleQuery \"SELECT 1\" to both\n    Then we should receive identical messages from both\n\n  @session-test\n  Scenario: Session management test\n    When we create session \"one\" to pg_doorman as \"example_user_1\" with password \"\" and database \"example_db\"\n    And we send SimpleQuery \"BEGIN\" to session \"one\"\n    And we send SimpleQuery \"SELECT pg_backend_pid()\" to session \"one\" and store backend_pid\n    # ... more steps\n</code></pre> <p>Available Rust test steps:</p> <p>Protocol comparison (sends to both PostgreSQL and pg_doorman): - <code>we login to postgres and pg_doorman as \"user\" with password \"pass\" and database \"db\"</code> - <code>we send SimpleQuery \"SQL\" to both</code> - <code>we send CopyFromStdin \"COPY ...\" with data \"...\" to both</code> - <code>we should receive identical messages from both</code></p> <p>Session management (for complex scenarios): - <code>we create session \"name\" to pg_doorman as \"user\" with password \"pass\" and database \"db\"</code> - <code>we send SimpleQuery \"SQL\" to session \"name\"</code> - <code>we send SimpleQuery \"SQL\" to session \"name\" and store backend_pid</code> - <code>we abort TCP connection for session \"name\"</code> - <code>we sleep 100ms</code></p> <p>Cancel request testing: - <code>we create session \"name\" ... and store backend key</code> - <code>we send SimpleQuery \"SQL\" to session \"name\" without waiting for response</code> - <code>we send cancel request for session \"name\"</code> - <code>session \"name\" should receive cancel error containing \"text\"</code></p>"},{"location":"tutorials/contributing/#adding-dependencies","title":"Adding Dependencies","text":"<p>If you need additional packages in the test environment, modify <code>tests/nix/flake.nix</code>: - Add Python packages to <code>pythonEnv</code> - Add system packages to <code>runtimePackages</code></p> <p>After modifying <code>flake.nix</code>, rebuild the image with <code>make local-build</code>.</p>"},{"location":"tutorials/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"tutorials/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow the Rust style guidelines</li> <li>Use meaningful variable and function names</li> <li>Add comments for complex logic</li> <li>Write tests for new functionality</li> </ul>"},{"location":"tutorials/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your feature or bugfix</li> <li>Make your changes and commit them with clear, descriptive messages</li> <li>Write or update tests as necessary</li> <li>Update documentation to reflect any changes</li> <li>Submit a pull request to the main repository</li> <li>Address any feedback from code reviews</li> </ol>"},{"location":"tutorials/contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you find a bug or have a feature request, please create an issue on the GitHub repository with:</p> <ul> <li>A clear, descriptive title</li> <li>A detailed description of the issue or feature</li> <li>Steps to reproduce (for bugs)</li> <li>Expected and actual behavior (for bugs)</li> </ul>"},{"location":"tutorials/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with your contribution, you can:</p> <ul> <li>Ask questions in the GitHub issues</li> <li>Reach out to the maintainers</li> </ul> <p>Thank you for contributing to PgDoorman!</p>"},{"location":"tutorials/installation/","title":"Installing PgDoorman","text":"<p>This guide covers different methods for installing and running PgDoorman on your system.</p>"},{"location":"tutorials/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Linux (recommended) or macOS</li> <li>PostgreSQL server (version 10 or higher)</li> <li>Sufficient memory for connection pooling (depends on expected load)</li> </ul>"},{"location":"tutorials/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"tutorials/installation/#pre-built-binaries-recommended","title":"Pre-built Binaries (Recommended)","text":"<p>The simplest way to install PgDoorman is to download a pre-built binary from the GitHub releases page.</p> <ol> <li>Download the appropriate binary for your platform</li> <li>Make the file executable: <code>chmod +x pg_doorman</code></li> <li>Move it to a directory in your PATH: <code>sudo mv pg_doorman /usr/local/bin/</code></li> <li>Create a configuration file (see Basic Usage for details)</li> </ol>"},{"location":"tutorials/installation/#building-from-source","title":"Building from Source","text":"<p>If you prefer to build from source, you'll need to clone the repository first:</p> <pre><code>git clone https://github.com/ozontech/pg_doorman.git\ncd pg_doorman\n</code></pre> <p>Then follow the instructions in the Contributing guide to build the project.</p>"},{"location":"tutorials/installation/#docker-installation","title":"Docker Installation","text":""},{"location":"tutorials/installation/#using-the-official-docker-image-recommended","title":"Using the Official Docker Image (Recommended)","text":"<p>PgDoorman provides an official Docker image that you can use directly:</p> <pre><code># Pull the official Docker image\ndocker pull ghcr.io/ozontech/pg_doorman\n\n# Run PgDoorman with your configuration\ndocker run -p 6432:6432 \\\n  -v /path/to/pg_doorman.toml:/etc/pg_doorman/pg_doorman.toml \\\n  --rm -t -i ghcr.io/ozontech/pg_doorman\n</code></pre>"},{"location":"tutorials/installation/#using-the-dockerfile","title":"Using the Dockerfile","text":"<p>You can build and run PgDoorman using Docker:</p> <pre><code># Build the Docker image\ndocker build -t pg_doorman -f Dockerfile .\n\n# Run PgDoorman with your configuration\ndocker run -p 6432:6432 \\\n  -v /path/to/pg_doorman.toml:/etc/pg_doorman/pg_doorman.toml \\\n  --rm -t -i pg_doorman\n</code></pre>"},{"location":"tutorials/installation/#using-nix-with-docker","title":"Using Nix with Docker","text":"<p>If you use Nix, you can build a Docker image:</p> <pre><code># Build the Docker image using Nix\nnix build .#dockerImage\n\n# Load the image into Docker\ndocker load -i result\n\n# Run PgDoorman with your configuration\ndocker run -p 6432:6432 \\\n  -v /path/to/pg_doorman.toml:/etc/pg_doorman/pg_doorman.toml \\\n  --rm -t -i pg_doorman\n</code></pre>"},{"location":"tutorials/installation/#using-docker-compose-or-podman-compose","title":"Using Docker Compose or Podman Compose","text":"<p>For a more complete setup including PostgreSQL, you can use Docker Compose or Podman Compose.</p> <p>A minimal compose configuration file is available in the repository examples directory.</p>"},{"location":"tutorials/installation/#running-with-docker-compose","title":"Running with Docker Compose","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"tutorials/installation/#running-with-podman-compose","title":"Running with Podman Compose","text":"<pre><code>podman-compose up -d\n</code></pre>"},{"location":"tutorials/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, you can verify that PgDoorman is running correctly by:</p> <ol> <li>Checking the process: <code>ps aux | grep pg_doorman</code></li> <li>Connecting to the admin console: <code>psql -h localhost -p 6432 -U admin pgdoorman</code></li> <li>Running <code>SHOW VERSION;</code> in the admin console</li> </ol>"},{"location":"tutorials/installation/#next-steps","title":"Next Steps","text":"<p>After installation, see the Basic Usage guide to configure and start using PgDoorman.</p>"},{"location":"tutorials/overview/","title":"PgDoorman Overview","text":""},{"location":"tutorials/overview/#what-is-pgdoorman","title":"What is PgDoorman?","text":"<p>PgDoorman is a high-performance PostgreSQL connection pooler based on PgCat. It acts as a middleware between your applications and PostgreSQL servers, efficiently managing database connections to improve performance and resource utilization.</p> <pre><code>graph LR\n    App1[Application A] --&gt; Pooler(PgDoorman)\n    App2[Application B] --&gt; Pooler\n    App3[Application C] --&gt; Pooler\n    Pooler --&gt; DB[(PostgreSQL)]</code></pre> <p>When an application connects to PgDoorman, it behaves exactly like a PostgreSQL server. Behind the scenes, PgDoorman either creates a new connection to the actual PostgreSQL server or reuses an existing connection from its pool, significantly reducing connection overhead.</p>"},{"location":"tutorials/overview/#key-benefits","title":"Key Benefits","text":"<ul> <li>Reduced Connection Overhead: Minimizes the performance impact of establishing new database connections</li> <li>Resource Optimization: Limits the number of connections to your PostgreSQL server</li> <li>Improved Scalability: Allows more client applications to connect to your database</li> <li>Connection Management: Provides tools to monitor and manage database connections</li> </ul>"},{"location":"tutorials/overview/#pooling-modes","title":"Pooling Modes","text":"<p>To maintain proper transaction semantics while providing efficient connection pooling, PgDoorman supports multiple pooling modes:</p>"},{"location":"tutorials/overview/#transaction-pooling","title":"Transaction Pooling","text":"<p>Recommended for most use cases</p> <p>In transaction pooling mode, a client is assigned a server connection only for the duration of a transaction. Once the transaction ends, the connection is released back into the pool.</p> <ul> <li>High Efficiency: Connections are shared between clients, allowing thousands of clients to share a small pool.</li> <li>Ideal for: Applications with many short-lived connections or those that don't rely on session state.</li> </ul>"},{"location":"tutorials/overview/#session-pooling","title":"Session Pooling","text":"<p>Useful for specific legacy needs</p> <p>In session pooling mode, each client is assigned a dedicated server connection for the entire duration of the client connection.</p> <ul> <li>Exclusive Allocation: The connection remains exclusively allocated to that client until disconnection.</li> <li>Support for Session Features: Ideal for applications that rely on temporary tables or session variables.</li> </ul>"},{"location":"tutorials/overview/#administration","title":"Administration","text":"<p>PgDoorman provides comprehensive tools for monitoring and management:</p> <ul> <li>Admin Console: A PostgreSQL-compatible interface for viewing statistics and managing the pooler</li> <li>Configuration Options: Extensive settings to customize behavior for your specific needs</li> <li>Monitoring: Detailed metrics about connection usage and performance</li> </ul> <p>For detailed information on managing PgDoorman, see the Admin Console documentation.</p>"},{"location":"tutorials/patroni-proxy/","title":"Patroni Proxy","text":"<p><code>patroni_proxy</code> is a specialized high-performance TCP proxy for Patroni-managed PostgreSQL clusters. Following the Unix philosophy of \"do one thing and do it well\", it focuses exclusively on TCP load balancing and failover for Patroni clusters.</p>"},{"location":"tutorials/patroni-proxy/#overview","title":"Overview","text":"<p>Unlike traditional solutions like HAProxy, <code>patroni_proxy</code> provides seamless connection management without disrupting existing connections during cluster topology changes. When a new replica is added or removed, only the affected connections are handled \u2014 all other connections continue working without interruption.</p>"},{"location":"tutorials/patroni-proxy/#key-features","title":"Key Features","text":""},{"location":"tutorials/patroni-proxy/#zero-downtime-connection-management","title":"Zero-Downtime Connection Management","text":"<p>The main advantage over HAProxy is that <code>patroni_proxy</code> does not terminate existing connections when the upstream configuration changes. This is critical for long-running transactions and connection-heavy applications.</p>"},{"location":"tutorials/patroni-proxy/#hot-upstream-updates","title":"Hot Upstream Updates","text":"<ul> <li>Automatic discovery of cluster members via Patroni REST API (<code>/cluster</code> endpoint)</li> <li>Periodic polling with configurable interval (<code>cluster_update_interval</code>)</li> <li>Immediate updates via HTTP API (<code>/update_clusters</code> endpoint)</li> <li>Configuration reload via SIGHUP signal without restart</li> </ul>"},{"location":"tutorials/patroni-proxy/#role-based-routing","title":"Role-Based Routing","text":"<p>Route connections based on PostgreSQL node roles:</p> Role Description <code>leader</code> Primary/master node <code>sync</code> Synchronous standby replicas <code>async</code> Asynchronous replicas <code>any</code> Any available node"},{"location":"tutorials/patroni-proxy/#intelligent-load-balancing","title":"Intelligent Load Balancing","text":"<ul> <li>Least Connections strategy for distributing connections across backends</li> <li>Connection counters are preserved during cluster updates</li> <li>Automatic exclusion of nodes with <code>noloadbalance</code> tag</li> </ul>"},{"location":"tutorials/patroni-proxy/#replication-lag-awareness","title":"Replication Lag Awareness","text":"<ul> <li>Configurable <code>max_lag_in_bytes</code> per port</li> <li>Automatic disconnection of clients when replica lag exceeds threshold</li> <li>Only affects replica connections (leader has no lag)</li> </ul>"},{"location":"tutorials/patroni-proxy/#member-state-filtering","title":"Member State Filtering","text":"<ul> <li>Only members with <code>state: \"running\"</code> are used as backends</li> <li>Members in <code>starting</code>, <code>stopped</code>, <code>crashed</code> states are automatically excluded</li> <li>Dynamic state changes are handled during periodic updates</li> </ul>"},{"location":"tutorials/patroni-proxy/#recommended-deployment-architecture","title":"Recommended Deployment Architecture","text":"<p>For optimal performance, we recommend a two-tier architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Application   \u2502     \u2502   Application   \u2502     \u2502   Application   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     patroni_proxy       \u2502  \u2190 Close to clients\n                    \u2502   (TCP load balancing)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502                       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   pg_doorman    \u2502     \u2502   pg_doorman    \u2502     \u2502   pg_doorman    \u2502  \u2190 Close to servers\n\u2502   (pooling)     \u2502     \u2502   (pooling)     \u2502     \u2502   (pooling)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   PostgreSQL    \u2502     \u2502   PostgreSQL    \u2502     \u2502   PostgreSQL    \u2502\n\u2502    (leader)     \u2502     \u2502 (sync replica)  \u2502     \u2502 (async replica) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>pg_doorman should be deployed close to PostgreSQL servers \u2014 it handles connection pooling, prepared statement caching, and protocol-level optimizations that benefit from low latency to the database</li> <li>patroni_proxy should be deployed close to application clients \u2014 it handles TCP routing and failover, distributing connections across the cluster without the overhead of connection pooling</li> </ul> <p>This separation allows each component to excel at its specific task while providing both high availability and optimal performance.</p>"},{"location":"tutorials/patroni-proxy/#configuration","title":"Configuration","text":"<p>Example <code>patroni_proxy.yaml</code>:</p> <pre><code># Cluster update interval in seconds (default: 3)\ncluster_update_interval: 3\n\n# HTTP API listen address for health checks and manual updates (default: 127.0.0.1:8009)\nlisten_address: \"127.0.0.1:8009\"\n\nclusters:\n  my_cluster:\n    # Patroni API endpoints (multiple for redundancy)\n    hosts:\n      - \"http://192.168.1.1:8008\"\n      - \"http://192.168.1.2:8008\"\n      - \"http://192.168.1.3:8008\"\n\n    # Optional: TLS configuration for Patroni API\n    # tls:\n    #   ca_cert: \"/path/to/ca.crt\"\n    #   client_cert: \"/path/to/client.crt\"\n    #   client_key: \"/path/to/client.key\"\n    #   skip_verify: false\n\n    ports:\n      # Primary/master connections\n      master:\n        listen: \"0.0.0.0:6432\"\n        roles: [\"leader\"]\n        host_port: 5432\n\n      # Read-only connections to replicas\n      replicas:\n        listen: \"0.0.0.0:6433\"\n        roles: [\"sync\", \"async\"]\n        host_port: 5432\n        max_lag_in_bytes: 16777216  # 16MB\n</code></pre>"},{"location":"tutorials/patroni-proxy/#configuration-options","title":"Configuration Options","text":"Option Default Description <code>cluster_update_interval</code> 3 Interval in seconds between Patroni API polls <code>listen_address</code> 127.0.0.1:8009 HTTP API listen address <code>clusters.&lt;name&gt;.hosts</code> - List of Patroni API endpoints <code>clusters.&lt;name&gt;.tls</code> - Optional TLS configuration for Patroni API <code>clusters.&lt;name&gt;.ports.&lt;name&gt;.listen</code> - Listen address for this port <code>clusters.&lt;name&gt;.ports.&lt;name&gt;.roles</code> - List of allowed roles <code>clusters.&lt;name&gt;.ports.&lt;name&gt;.host_port</code> - PostgreSQL port on backend hosts <code>clusters.&lt;name&gt;.ports.&lt;name&gt;.max_lag_in_bytes</code> - Maximum replication lag (optional)"},{"location":"tutorials/patroni-proxy/#usage","title":"Usage","text":""},{"location":"tutorials/patroni-proxy/#starting-patroni_proxy","title":"Starting patroni_proxy","text":"<pre><code># Start with configuration file\npatroni_proxy /path/to/patroni_proxy.yaml\n\n# With debug logging\nRUST_LOG=debug patroni_proxy /path/to/patroni_proxy.yaml\n</code></pre>"},{"location":"tutorials/patroni-proxy/#configuration-reload","title":"Configuration Reload","text":"<p>Reload configuration without restart (add/remove ports, update hosts):</p> <pre><code>kill -HUP $(pidof patroni_proxy)\n</code></pre>"},{"location":"tutorials/patroni-proxy/#manual-cluster-update","title":"Manual Cluster Update","text":"<p>Trigger immediate update of all cluster members via HTTP API:</p> <pre><code>curl http://127.0.0.1:8009/update_clusters\n</code></pre>"},{"location":"tutorials/patroni-proxy/#http-api","title":"HTTP API","text":"Endpoint Method Description <code>/update_clusters</code> GET Trigger immediate update of all cluster members <code>/</code> GET Health check (returns \"OK\")"},{"location":"tutorials/patroni-proxy/#comparison-with-haproxy-confd","title":"Comparison with HAProxy + confd","text":"Feature patroni_proxy HAProxy + confd Connection preservation on update \u2705 Yes \u274c No (reload drops connections) Hot upstream updates \u2705 Native \u26a0\ufe0f Requires confd + reload Replication lag awareness \u2705 Built-in \u26a0\ufe0f Requires custom checks Configuration complexity \u2705 Single YAML \u274c Multiple configs Resource usage \u2705 Lightweight \u26a0\ufe0f HAProxy + confd processes Role-based routing \u2705 Native \u26a0\ufe0f Requires custom templates"},{"location":"tutorials/patroni-proxy/#building","title":"Building","text":"<pre><code># Build release binary\ncargo build --release --bin patroni_proxy\n\n# Run tests\ncargo test --test patroni_proxy_bdd\n</code></pre>"},{"location":"tutorials/patroni-proxy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/patroni-proxy/#no-backends-available","title":"No backends available","text":"<p>If you see warnings like <code>no backends available</code>, check:</p> <ol> <li>Patroni API is accessible from patroni_proxy host</li> <li>Cluster members have <code>state: \"running\"</code></li> <li>Roles in configuration match actual member roles</li> <li>If using <code>max_lag_in_bytes</code>, check replica lag values</li> </ol>"},{"location":"tutorials/patroni-proxy/#connection-drops-after-update","title":"Connection drops after update","text":"<p>This should not happen with patroni_proxy. If connections are being dropped:</p> <ol> <li>Check if the backend host was actually removed from the cluster</li> <li>Verify <code>max_lag_in_bytes</code> threshold is not being exceeded</li> <li>Enable debug logging to see detailed connection lifecycle</li> </ol>"},{"location":"tutorials/troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps you resolve common issues when using PgDoorman.</p>"},{"location":"tutorials/troubleshooting/#todo","title":"TODO","text":"<p>Still having issues?</p> <p>If you encounter a problem not listed here, please open an issue on GitHub.</p>"}]}